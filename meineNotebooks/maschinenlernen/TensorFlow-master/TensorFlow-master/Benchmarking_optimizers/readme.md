Trying to benchmark all the optimizers used in deep learning 


## Networks
- FC
- CNN 
- RNN 
- LSTM 


## Optimizers to test
- adadelta
- adagrad
- adam
- ftrl
- momentum
- rmsprop
- sgd 

## Things to check 
- learning_rate behaviour with-in each optimizer
- time taken to run each optimizer
- hyper-parameters to tune (check for one)
- maximum training, testing and validation accuracy achieved
- graphs 

## Datasets
- MNIST
- CIFAR10
- WONDERLAND.txt
- CIFAR100 if time permits 

# Later works:

## Networks
- LeNet
- AlexNet
- FMP 
