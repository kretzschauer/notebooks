{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronales Netz\n",
    "Quelle: [BogoToBogo](https://www.bogotobogo.com/python/python_Neural_Networks_Backpropagation_for_XOR_using_one_hidden_layer.php), mit vielen Tutorials zu Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# die Aktivierungsfunktion und ihre erste Ableitung\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "def tanh_abl(x):\n",
    "    return 1.0-x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Aktivierungsfunktion ist hier $f(x)= \\tanh x =  \\frac{sinh(x)}{cosh(x)}= \\frac{e^x -e^{-x}}{e^x + e^{-x}}=1-\\frac{2}{e^{2x}+1}$  gewählt, weil es die besten Lern-Ergebnisse  lieferte. Im Bild sieht man grün die Funktion und blau die Ableitungsfunktion $f'(x)= 1- f(x)² $. NACHRECHNEN! ![Aktivierungsfunktionen tanh](images/aktivierungsfunktionen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spezielles Netzwerk für XOR mit zwei Eingaben, einer Ausgabe und einer versteckten Schicht mit zwei Knoten: Layers = [2,2,1].\n",
    "In der Quelle ist das Programm allgemeiner dargestellt, so dass es für andere Netze angewendet werden kann.\n",
    "![Bild vom Netzwerk](images/NeuralNetworksDiagram00.png)\n",
    "\n",
    "\n",
    "Die Korrektur der Gewichte mittels Backpropagation ist gut auf [Wikipedia](https://de.wikipedia.org/wiki/Backpropagation \"Wikipedia\") beschrieben\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronalNetwork:\n",
    "    def __init__(self):\n",
    "        layers=[2,2,1]\n",
    "        self.weights=[] #Gewichte zufaellig festlegen\n",
    "        w=2*np.random.random((3,3))-1 #liegen zwischen -1 und 1\n",
    "        self.weights.append(w)\n",
    "        w=2*np.random.random((3,1))-1\n",
    "        self.weights.append(w)\n",
    "        self.fehler=0\n",
    "        \n",
    "    def prepareInput(self,inListe):\n",
    "        # setzt eine Spalte mit Einsen als Bias vor die Eingabe\n",
    "        eingabe=np.array(inListe)\n",
    "        X        = np.ones((len(eingabe),len(eingabe[0])+1))\n",
    "        X[:,1:] = eingabe\n",
    "        return X\n",
    "        \n",
    "    def fit(self,X,y,lernrate=0.1,epoches=1000):\n",
    "        for k in range(epoches):\n",
    "            i=np.random.randint(X.shape[0])#zufaellig eine Eingabe waehlen\n",
    "            a=[X[i]] # eine Liste der Ausgaben der einzelnen Schichten \n",
    "            for j in range(len(self.weights)):\n",
    "                hidden_in  = np.dot(a[j],self.weights[j]) # Eingabe mal Gewichte\n",
    "                hidden_out = tanh(hidden_in)              # Aktivierungsfunktion\n",
    "                a.append(hidden_out)                      # anhaengen an die Liste\n",
    "            error  = y[i]-a[-1]\n",
    "            self.fehler +=np.sum(np.square(error))  #zur Kontrolle der quadr. Fehler   \n",
    "            # Backpropagation, Fehler berechnen und an die Fehlerliste anhaengen\n",
    "            deltas = [error * tanh_abl(a[-1])] #Fehler der Ausgabeschicht\n",
    "            for j in range(len(a)-2,0,-1):\n",
    "                deltas.append(deltas[-1].dot(self.weights[j].T)*tanh_abl(a[j]))\n",
    "                # Fehleranteile der vorherigen Schichten\n",
    "            deltas.reverse()            \n",
    "            # Korrektur der Gewichte \n",
    "            for i in range(len(self.weights)):\n",
    "                layer=np.array(a[i])\n",
    "                layer=layer[:,None]#Umwandeln in eine nx1 Matrix \n",
    "                delta=np.array(deltas[i])\n",
    "                delta=delta[None,:]#Umwandeln in eine 1xn-Matrix               \n",
    "                self.weights[i]+=lernrate*np.dot(layer,delta)\n",
    "          \n",
    "    def predict(self,x):      \n",
    "        for i in range(len(self.weights)):\n",
    "            x=tanh(np.dot(x,self.weights[i]))            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[0. 0.] [0.20073039]\n",
      "[0. 1.] [0.46479922]\n",
      "[1. 0.] [0.49214208]\n",
      "[1. 1.] [0.59272227]\n",
      "27.908655689337433\n",
      "\n",
      "2\n",
      "[0. 0.] [0.08815079]\n",
      "[0. 1.] [0.64972454]\n",
      "[1. 0.] [0.64952198]\n",
      "[1. 1.] [0.78697971]\n",
      "18.872581504251382\n",
      "\n",
      "3\n",
      "[0. 0.] [0.17693491]\n",
      "[0. 1.] [0.71379002]\n",
      "[1. 0.] [0.7177991]\n",
      "[1. 1.] [0.81133927]\n",
      "22.935758299658854\n",
      "\n",
      "4\n",
      "[0. 0.] [0.09797665]\n",
      "[0. 1.] [0.69962784]\n",
      "[1. 0.] [0.69392853]\n",
      "[1. 1.] [0.78790025]\n",
      "20.39517811111785\n",
      "\n",
      "5\n",
      "[0. 0.] [0.14077961]\n",
      "[0. 1.] [0.7675278]\n",
      "[1. 0.] [0.75418205]\n",
      "[1. 1.] [0.82970244]\n",
      "19.593355379846884\n",
      "\n",
      "6\n",
      "[0. 0.] [0.00132571]\n",
      "[0. 1.] [0.69553472]\n",
      "[1. 0.] [0.68282239]\n",
      "[1. 1.] [0.76581158]\n",
      "16.955769915106657\n",
      "\n",
      "7\n",
      "[0. 0.] [0.05589213]\n",
      "[0. 1.] [0.76600475]\n",
      "[1. 0.] [0.75824782]\n",
      "[1. 1.] [0.82124858]\n",
      "15.283330980785896\n",
      "\n",
      "8\n",
      "[0. 0.] [0.01095215]\n",
      "[0. 1.] [0.67160428]\n",
      "[1. 0.] [0.66377257]\n",
      "[1. 1.] [0.72051247]\n",
      "18.94475867161495\n",
      "\n",
      "9\n",
      "[0. 0.] [0.02948169]\n",
      "[0. 1.] [0.7005866]\n",
      "[1. 0.] [0.69562086]\n",
      "[1. 1.] [0.74007868]\n",
      "17.955348720038106\n",
      "\n",
      "10\n",
      "[0. 0.] [0.10343366]\n",
      "[0. 1.] [0.70380844]\n",
      "[1. 0.] [0.70410534]\n",
      "[1. 1.] [0.73162273]\n",
      "18.517754101127213\n",
      "\n",
      "11\n",
      "[0. 0.] [0.07672419]\n",
      "[0. 1.] [0.69533394]\n",
      "[1. 0.] [0.69965951]\n",
      "[1. 1.] [0.71541854]\n",
      "17.870027998941765\n",
      "\n",
      "12\n",
      "[0. 0.] [0.0048118]\n",
      "[0. 1.] [0.66706771]\n",
      "[1. 0.] [0.68108438]\n",
      "[1. 1.] [0.68022975]\n",
      "17.419931107237762\n",
      "\n",
      "13\n",
      "[0. 0.] [0.05055274]\n",
      "[0. 1.] [0.58974708]\n",
      "[1. 0.] [0.61920673]\n",
      "[1. 1.] [0.56338194]\n",
      "19.585787495533832\n",
      "\n",
      "14\n",
      "[0. 0.] [0.15513086]\n",
      "[0. 1.] [0.72570753]\n",
      "[1. 0.] [0.72604316]\n",
      "[1. 1.] [0.66405136]\n",
      "17.8599351996464\n",
      "\n",
      "15\n",
      "[0. 0.] [0.1868695]\n",
      "[0. 1.] [0.74396985]\n",
      "[1. 0.] [0.69579986]\n",
      "[1. 1.] [0.57295775]\n",
      "14.939859131658515\n",
      "\n",
      "16\n",
      "[0. 0.] [0.11412244]\n",
      "[0. 1.] [0.68134777]\n",
      "[1. 0.] [0.64366295]\n",
      "[1. 1.] [0.36196947]\n",
      "11.842573662058186\n",
      "\n",
      "17\n",
      "[0. 0.] [0.0160518]\n",
      "[0. 1.] [0.56134855]\n",
      "[1. 0.] [0.55073832]\n",
      "[1. 1.] [0.07102086]\n",
      "7.941807613738419\n",
      "\n",
      "18\n",
      "[0. 0.] [0.07732213]\n",
      "[0. 1.] [0.80268589]\n",
      "[1. 0.] [0.82372654]\n",
      "[1. 1.] [0.43580173]\n",
      "6.216885707624877\n",
      "\n",
      "19\n",
      "[0. 0.] [0.05719077]\n",
      "[0. 1.] [0.78135804]\n",
      "[1. 0.] [0.70053178]\n",
      "[1. 1.] [0.10643308]\n",
      "4.31056495096992\n",
      "\n",
      "20\n",
      "[0. 0.] [0.09364449]\n",
      "[0. 1.] [0.79985393]\n",
      "[1. 0.] [0.81602319]\n",
      "[1. 1.] [0.10077244]\n",
      "3.3395544381811906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#np.random.seed(0)#liefert reproduzierbare Ergebnisse\n",
    "\n",
    "nn=NeuronalNetwork()\n",
    "#Eingabevektor, die Vorbereitung erfolgt in einer Extra -Funktion des Netzes\n",
    "eingabe=[[0, 0], [0, 1],[1, 0], [1, 1]]\n",
    "#Die Ziel-Ausgabe\n",
    "y = np.array([0, 1, 1, 0])\n",
    "X=nn.prepareInput(eingabe)\n",
    "#Test des Netzes\n",
    "for i in range(20):\n",
    "    print(i+1)\n",
    "    nn.fit(X,y,0.1,100)\n",
    "    for e in X:\n",
    "        print(e[1:],nn.predict(e))\n",
    "    print(nn.fehler)\n",
    "    if nn.fehler<1:\n",
    "        break\n",
    "    nn.fehler=0\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
