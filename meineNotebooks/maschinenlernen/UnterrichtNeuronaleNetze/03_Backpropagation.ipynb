{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lernen im mehrschichtigen Netz\n",
    "## Entwurf des Netzes\n",
    "Das folgende Netz soll programmiert werden. Es besitzt zwei Eingabeneuronen plus ein Bias-Neuron in der Eingabeschicht, drei Neuronen in der verdeckten Schicht und zwei Neuronen für die Ausgabe. Die Bezeichnungen für die Gewichte sind so, wie sie im folgenden benutzt werden, die Indizes geben an \"woher\" \"wohin\".\n",
    "![Netzaufbau](bilder/NeuronNetz332.png \"Titel\")\n",
    "Mit diesem Netz soll die XOR-Funktion berechnet und gelernt werden.\n",
    "Für die folgenden Beispielrechnungen werden die folgenden Vereinfachungen gemacht:\n",
    "* alle Gewichte werden bis auf zwei Ausnahmen  mit 0,1 initialisiert\n",
    "* der Lernfaktor beträgt ebenfalls 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Berechnung der Netzeingabe\n",
    "Als Eingabe soll der Vektor (1, 0, 1) dienen, die erste 1 ist vom Bias-Neuron, die beiden anderen repräsentieren x<sub>1</sub> und x<sub>2</sub>.\n",
    "Für den Knoten 1 der versteckten Schicht berechnet sich die Eingabe wie folgt:\n",
    "<span class=\"math\">$$HI_1 =  x_0 \\cdot w^{IH}_{01} + x_1 \\cdot w^{IH}_{11} + x_2 \\cdot w^{IH}_{21} = 0,3 $$</span> \n",
    "Das lässt sich sehr elegant als Produkt der Gewichtsmatrix mit dem Eingabevektor beschreiben und implementieren und ergibt HI, die Eingabe in die versteckte Schicht:\n",
    "<span class=\"math\">$$ IN\\cdot W^{IH} = HI  $$</span>\n",
    "\n",
    "<math>$$\\begin{pmatrix}  1  & 0 & 1  \\end{pmatrix} \\cdot \\begin{pmatrix} 0,1 & 0,2 & 0,1 \\\\ 0,1 & 0,1 & 0,1 \\\\ 0,1 & 0,1 & 0,1\\end{pmatrix}  \n",
    "      =  \\begin{pmatrix}0,2 & 0,3 & 0,2 \\end{pmatrix} $$</math>  \n",
    "    \n",
    "Dabei kann die erste Komponente des Ergebnisvektors als Eingabe in den Bias-Knoten der verdeckten Schicht interpretiert werden und wird ignoriert. In der Implementierung haben die Matrizen aus Gründen der Einheitlichkeit stets diese Dimensionen und auch dabei werden einzelne Ergebnisse ignoriert.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Berechnung der Aktivierung und der Ausgabe\n",
    "Auf die Eingaben der verdeckten Schicht wird nun jeweils die Stufenfunktion angewendet und man erhält HO, die Ausgabe der versteckten Schicht:\n",
    "<math>$$ HO = \\sigma(HI) = \\sigma\\begin{pmatrix}  0,2  & 0,3  & 0,2  \\end{pmatrix} =  \\begin{pmatrix}  0,5498 & 0,5744 &  0,5498 \\end{pmatrix} $$</math>\n",
    "\n",
    "Mit der Ausgabe der versteckten Schicht berechnet man die Eingabe in das Ausgabeneuron FI (final in).\n",
    "\n",
    "$$ HO \\cdot W^{HO}= FI  $$\n",
    "\n",
    "$$\\begin{pmatrix}  0,5498 & 0,5744 &  0,5498 \\end{pmatrix} \\cdot \n",
    "     \\begin{pmatrix}  0,2 & 0,1  \\\\ 0,1 & 0,1  \\\\ 0,1 & 0,1  \\end{pmatrix} =  \\begin{pmatrix}0,2224 & 0,1674 \\end{pmatrix} $$ \n",
    "\n",
    "Auch auf diese Eingabe wird wieder die Sigmoid-Funktion angewendet, man erhält FO (final out):\n",
    "\n",
    "$$ FO = \\sigma(FI) \\begin{pmatrix}0,2224 & 0,1674 \\end{pmatrix} = \\begin{pmatrix}0,5554 & 0,5418 \\end{pmatrix}  $$\n",
    "\n",
    "Damit hat die Eingabe einmal das Netz durchlaufen und verursacht einen Fehler, der dann zur Korrektur der Gewichte benutzt wird.\n",
    "$$ E = \\frac {1}{2} (t- FO)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "### Gewichtskorrektur aus der Outputschicht heraus\n",
    "Wir nehmen an, dass unser Netz in der Ausgabeschicht einen Fehler E gemacht hat, der sich aus der Differenz des gewünschten Wertes t und der berechneten Ausgabe FO ergibt. \n",
    "\n",
    "$$ E = \\frac {1}{2} (t- FO)^2$$\n",
    "\n",
    "und zur Erinnerung setzt sich FO zusammen aus:\n",
    "\n",
    "$$FO = \\sigma(FI)= \\sigma(HO \\cdot W^{HO}) $$\n",
    "\n",
    "Um den Einfluss der einzelnen Gewichte auf den Fehler zu bestimmen und diese entsprechend zu korrigieren, wird der Fehler nach dem jeweiligen Gewicht abgeleitet. Damit erhält man den Gradienten des Fehlers, das ist ein Vektor, der die Richtung der größten Steigung der Fehlerfunktion angibt. Die Korrektur muss dann also entgegengerichtet zu diesem Vektor erfolgen. \n",
    "Allgemein gilt:\n",
    "$$\\Delta W^{HO}= -\\eta \\frac {\\delta E}{\\delta w^{HO}}$$\n",
    "\n",
    "Für die Ableitung muss nun mehrfach die Kettenregel angewendet werden.\n",
    "Für eine Komponente dieses Vektors soll diese Ableitung berechnet werden:\n",
    "$$\\frac {\\delta E}{\\delta w^{HO}_{ij}}= \\frac {\\delta(\\frac {1}{2} (t- FO)^2)}{\\delta w^{HO}_{ij}} $$\n",
    "\n",
    "mit der Kettenregel ergibt sich\n",
    "\n",
    "$$\\frac {\\delta E}{\\delta w^{HO}_{ij}}=-(t-FO) \\cdot  \\frac {\\delta FO}{\\delta w^{HO}_{ij}} $$\n",
    "\n",
    "mit $$FO = \\sigma(FI)$$ und \n",
    "\n",
    "$$ \\sigma '(FI) = \\sigma(FI)\\cdot (1-\\sigma(FI))$$\n",
    "\n",
    "für die Sigmoid-Funktion bekommt man\n",
    "\n",
    "$$\\frac {\\delta E}{\\delta w^{HO}_{ij}}=-(t-FO) \\cdot  \\frac {\\delta (\\sigma(FI))}{\\delta w^{HO}_{ij}} $$\n",
    "\n",
    "$$\\frac {\\delta E}{\\delta w^{HO}_{ij}}=-(t-FO) \\cdot \\sigma(FI)\\cdot (1- \\sigma(FI))\\cdot \\frac {\\delta (FI)}{\\delta w^{HO}_{ij}} $$\n",
    "\n",
    "oder \n",
    "\n",
    "$$\\frac {\\delta E}{\\delta w^{HO}_{ij}}=-(t-FO) \\cdot FO\\cdot (1- FO)\\cdot \\frac {\\delta (FI)}{\\delta w^{HO}_{ij}} $$\n",
    "\n",
    "Die Eingabe in die Outputschicht FI schließlich hängt direkt von den Gewichten ab, in unserem Beispiel gilt\n",
    "\n",
    "$$FI = w^{HO}_{10}\\cdot HO_0 + w^{HO}_{11}\\cdot HO_1 +w^{HO}_{12}\\cdot HO_2 $$\n",
    "\n",
    "In die Ableitung geht damit nur der Summand ein, in dem das Gewicht, nach dem abgeleitet wird, steht. \n",
    "\n",
    "$$\\frac {\\delta E}{\\delta w^{HO}_{10}}=-(t-FO) \\cdot FO\\cdot (1- FO)\\cdot \\frac {\\delta (FI)}{\\delta w^{HO}_{10}} $$\n",
    "\n",
    "$$\\frac {\\delta E}{\\delta w^{HO}_{10}}=-(t-FO) \\cdot FO\\cdot (1- FO)\\cdot HO_0 $$\n",
    "\n",
    "Für die Gewichtskorrektur wird noch mit (-1) und dem Lernfaktor multipliziert. \n",
    "\n",
    "$$\\Delta w^{HO}_{10}= \\eta \\cdot (t-FO) \\cdot FO\\cdot (1- FO)\\cdot HO_0 \\cdot $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gewichtskorrektur aus der Hidden-Schicht heraus  muss neu gemacht werden\n",
    "\n",
    "Analog wie oben geht man vor um die Korrektur von w^{IH} zu berechnen. Auch hier muss der Anteil der einzelnen Gewichte am Fehler bestimmt werden. Da man davon ausgeht, dass jeder der Knoten der Hidden-Schicht entsprechend des Gewichts w^{HO} zum Gesamtfehler beigetragen hat, wichtet man den Fehler dieser Knoten mit diesen Gewichten. Es gilt für den i-ten Knoten der Hidden-Schicht\n",
    "\n",
    "$$ delta^{HO}_i = der_i \\cdot w^{HO}_{1i} \\cdot delta^{FO}$$\n",
    "\n",
    "Da es hier insgesamt 9 Gewichtskorrekturen gibt, schreibt sich das besser als Matrizenmultiplikation:\n",
    "\n",
    "$$\\Delta w^{IH}_{ji}= \\eta \\cdot \\begin{pmatrix} -0,0667 \\\\ -0,0131 \\\\ -0,0131 \\end{pmatrix} \\cdot \n",
    "     \\begin{pmatrix}  1  \\\\ 0  \\\\ 0  \\end{pmatrix}^T \n",
    "     = \\eta \\cdot \\begin{pmatrix} -0,0667 \\\\ -0,0131 \\\\ -0,0131 \\end{pmatrix} \\cdot \n",
    "     \\begin{pmatrix}  1  & 0  & 0  \\end{pmatrix}\n",
    "     = \\eta \\cdot \\begin{pmatrix}-0,0667 & 0 & 0\\\\ -0,0131 & 0 & 0 \\\\-0,0131 &0 &0\\end{pmatrix} $$ \n",
    "\n",
    "Allgemein gilt also:\n",
    "\n",
    "$$\\Delta w^{IH}_{ji}= \\eta \\cdot delta^{HO}_j  \\cdot  IN_i^T  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rechenbeispiel als Excel-Tabelle:\n",
    "hier gelten:\n",
    "$$der = FO\\cdot (1- FO) $$und für die Output-Schicht:\n",
    "\n",
    "$$delta = (t-FO)\\cdot der$$\n",
    "sowie \n",
    "$$\\Delta w^{HO}_{10}= \\eta \\cdot delta \\cdot HO_0$$\n",
    "![Berechnung](bilder/ergebnisseXOR101.png \"Rechnung\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
