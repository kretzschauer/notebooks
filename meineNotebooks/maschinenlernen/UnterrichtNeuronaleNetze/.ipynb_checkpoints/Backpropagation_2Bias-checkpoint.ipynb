{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lernen im mehrschichtigen Netz\n",
    "## Entwurf des Netzes\n",
    "Das folgende Netz soll programmiert werden. Es besitzt zwei Eingabeneuronen plus ein Bias-Neuron in der Eingabeschicht, zwei Neuronen und ein Bias-Neuron in der verdeckten Schicht und ein Neuron für die Ausgabe. Die Bezeichnungen für die Gewichte sind so, wie sie im folgenden benutzt werden, die Indizes geben an \"woher\" \"wohin\".\n",
    "![Netzaufbau](bilder/NeuronNetz321.png \"Titel\")\n",
    "Mit diesem Netz soll die XOR-Funktion berechnet und gelernt werden.\n",
    "Für die folgenden Beispielrechnungen werden die folgenden Vereinfachungen gemacht:\n",
    "* alle Gewichte werden mit festen Werten initialisiert\n",
    "\n",
    "$$W^{IH} = \\begin{pmatrix} 0 & 0,2 & 0,1 \\\\ 0 & -0,2 & 0,1 \\\\ 0 & 0,1 & 0,1\\end{pmatrix}  $$\n",
    "\n",
    "$$ W^{HO} = \\begin{pmatrix}  0,2  \\\\ 0,1   \\\\ 0,1   \\end{pmatrix} $$\n",
    "\n",
    "* der Lernfaktor beträgt 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Berechnung der Netzeingabe\n",
    "Als Eingabe soll der Vektor (1, 0, 1) dienen, die erste 1 ist vom Bias-Neuron, die beiden anderen repräsentieren x<sub>1</sub> und x<sub>2</sub>.\n",
    "Für den Knoten 1 der versteckten Schicht berechnet sich die Eingabe wie folgt:\n",
    "<span class=\"math\">$$HI_1 =  x_0 \\cdot w_{01} + x_1 \\cdot w_{11} + x_2 \\cdot w_{21} = 0,3 $$</span> \n",
    "Das lässt sich sehr elegant als Produkt der Gewichtsmatrix mit dem Eingabevektor beschreiben und implementieren und ergibt HI, die Eingabe in die versteckte Schicht:\n",
    "<span class=\"math\">$$ IN\\cdot W^{IH} = HI  $$</span>\n",
    "\n",
    "<math>$$\\begin{pmatrix}  1  & 0 & 1  \\end{pmatrix} \\cdot \\begin{pmatrix} 0 & 0,2 & 0,1 \\\\ 0 & -0,2 & 0,1 \\\\ 0 & 0,1 & 0,1\\end{pmatrix}  \n",
    "      =  \\begin{pmatrix}0 & 0,3 & 0,2 \\end{pmatrix} $$</math>  \n",
    "    \n",
    "Dabei kann die erste Komponente des Ergebnisvektors als Eingabe in den Bias-Knoten der verdeckten Schicht interpretiert werden und wird ignoriert. In der Implementierung haben die Matrizen aus Gründen der Einheitlichkeit stets diese Dimensionen und auch dabei werden einzelne Ergebnisse ignoriert.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Berechnung der Aktivierung und der Ausgabe\n",
    "Auf die Eingaben der verdeckten Schicht wird nun jeweils die Stufenfunktion angewendet und man erhält die Aktivierung der versteckten Schicht. Für die Ausgabe der versteckten Schicht wird der erste Wert durch 1 als Ausgabe des Bias-Neutrons ersetzt.\n",
    "\n",
    "$$\\sigma(HI) = \\sigma\\begin{pmatrix}  0  & 0,3  & 0,2  \\end{pmatrix} =  \\begin{pmatrix}  0,5 & 0,5744 &  0,5498 \\end{pmatrix} $$\n",
    "$$ HO = \\begin{pmatrix} 1 & 0,5744 &  0,5498 \\end{pmatrix}$$\n",
    "Mit der Ausgabe der versteckten Schicht berechnet man die Eingabe in das Ausgabeneuron FI (final in).\n",
    "\n",
    "$$ HO \\cdot W^{HO}= FI  $$\n",
    "\n",
    "$$\\begin{pmatrix}  1 & 0,5744 &  0,5498 \\end{pmatrix} \\cdot \n",
    "     \\begin{pmatrix}  0,2  \\\\ 0,1   \\\\ 0,1   \\end{pmatrix} =  \\begin{pmatrix}0,3124\\end{pmatrix} $$ \n",
    "\n",
    "Auch auf diese Eingabe wird wieder die Sigmoid-Funktion angewendet, man erhält FO (final out):\n",
    "\n",
    "$$ FO = \\sigma(FI) =\\sigma\\begin{pmatrix}0,3124 \\end{pmatrix} = \\begin{pmatrix}0,5775\\end{pmatrix}  $$\n",
    "\n",
    "Damit hat die Eingabe einmal das Netz durchlaufen und verursacht einen Fehler, der dann zur Korrektur der Gewichte benutzt wird.\n",
    "$$ E = \\frac {1}{2} (t- FO)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "### Gewichtskorrektur aus der Outputschicht heraus\n",
    "Wir nehmen an, dass unser Netz in der Ausgabeschicht einen Fehler E gemacht hat, der sich aus der Differenz des gewünschten Wertes t und der berechneten Ausgabe FO ergibt. \n",
    "\n",
    "$$ E = \\frac {1}{2} (t- FO)^2$$\n",
    "\n",
    "Zur Erinnerung: so setzt sich FO zusammen:\n",
    "\n",
    "$$FO = \\sigma(FI)= \\sigma(HO \\cdot W^{HO}) $$\n",
    "\n",
    "Um den Einfluss der einzelnen Gewichte auf den Fehler zu bestimmen und diese entsprechend zu korrigieren, wird der Fehler nach dem jeweiligen Gewicht abgeleitet. Damit erhält man den Gradienten des Fehlers, das ist ein Vektor, der die Richtung der größten Steigung der Fehlerfunktion angibt. Die Korrektur muss dann also entgegengerichtet zu diesem Vektor erfolgen. \n",
    "Allgemein gilt:\n",
    "$$\\Delta w^{HO}= -\\eta \\frac {\\delta E}{\\delta w^{HO}}$$\n",
    "\n",
    "Für eine Komponente dieses Vektors soll diese Ableitung berechnet werden:\n",
    "$$\\frac {\\delta E}{\\delta w^{HO}_{ij}}= \\frac {\\delta(\\frac {1}{2} (t- FO)^2)}{\\delta w^{HO}_{ij}} $$\n",
    "\n",
    "mit der Kettenregel ergibt sich\n",
    "\n",
    "$$\\frac {\\delta E}{\\delta w^{HO}_{ij}}=-(t-FO) \\cdot  \\frac {\\delta FO}{\\delta w^{HO}_{ij}} $$\n",
    "\n",
    "mit $$FO = \\sigma(FI)$$ und \n",
    "\n",
    "$$ \\sigma '(FI) = \\sigma(FI)\\cdot (1-\\sigma(FI))= FO \\cdot (1-(FO)) = der$$\n",
    "\n",
    "für die Sigmoid-Funktion bekommt man\n",
    "\n",
    "$$\\frac {\\delta E}{\\delta w^{HO}_{ij}}=-(t-FO) \\cdot  \\frac {\\delta (\\sigma(FI))}{\\delta w^{HO}_{ij}} $$\n",
    "\n",
    "$$\\frac {\\delta E}{\\delta w^{HO}_{ij}}=-(t-FO) \\cdot \\sigma(FI)\\cdot (1- \\sigma(FI))\\cdot \\frac {\\delta (FI)}{\\delta w^{HO}_{ij}} $$\n",
    "\n",
    "oder \n",
    "\n",
    "$$\\frac {\\delta E}{\\delta w^{HO}_{ij}}=-(t-FO) \\cdot FO\\cdot (1- FO)\\cdot \\frac {\\delta (FI)}{\\delta w^{HO}_{ij}} $$\n",
    "\n",
    "Die Eingabe in die Outputschicht FI (hier mit Index 1) schließlich hängt direkt von den Gewichten ab, in unserem Beispiel gilt\n",
    "$$FI =HO_0 \\cdot w^{HO}_{01}  + HO_1 \\cdot w^{HO}_{11}+HO_2 \\cdot w^{HO}_{21}$$\n",
    "\n",
    "wobei HO<sub>0</sub> stets 1 ist (BIAS).\n",
    "\n",
    "In die Ableitung geht damit nur der Summand ein, in dem das Gewicht, nach dem abgeleitet wird, steht. \n",
    "\n",
    "$$\\frac {\\delta E}{\\delta w^{HO}_{01}}=-(t-FO) \\cdot FO\\cdot (1- FO)\\cdot \\frac {\\delta (HO_0 \\cdot w^{HO}_{01}  + HO_1 \\cdot w^{HO}_{11}+HO_2 \\cdot w^{HO}_{21})}{\\delta w_{01}} $$\n",
    "\n",
    "$$\\frac {\\delta E}{\\delta w^{HO}_{01}}=-(t-FO) \\cdot FO\\cdot (1- FO)\\cdot HO_0 $$\n",
    "\n",
    "Für die Gewichtskorrektur wird noch mit (-1) und dem Lernfaktor multipliziert. \n",
    "\n",
    "$$\\Delta w^{HO}_{01}= \\eta \\cdot (t-FO) \\cdot FO\\cdot (1- FO)\\cdot HO_0 \\cdot $$\n",
    "Analog gilt für die anderen Gewichte:\n",
    "$$\\Delta w^{HO}_{11}= \\eta \\cdot (t-FO) \\cdot FO\\cdot (1- FO)\\cdot HO_1 \\cdot $$\n",
    "\n",
    "$$\\Delta w^{HO}_{21}= \\eta \\cdot (t-FO) \\cdot FO\\cdot (1- FO)\\cdot HO_2 \\cdot $$\n",
    "\n",
    "so dass es sich zu einer Matrizenmultiplikation zusammenfassen lässt:\n",
    "$$\\Delta w^{HO} = \\eta \\cdot HO^{T}\\cdot(t-FO) \\cdot FO\\cdot (1- FO)$$\n",
    "Für unsere Beispieleingabe ergibt sich dann\n",
    "$$\\Delta w^{HO}= 1 \\cdot \\begin{pmatrix}  1 & 0,5744 &  0,5498 \\end{pmatrix}^T \\cdot(1-0,5775) \\cdot 0,5775\\cdot (1- 0,5775) =\\begin{pmatrix}  0,1031 \\\\ 0,0592 \\\\  0,0567 \\end{pmatrix}  $$\n",
    "und damit als neue Gewichte\n",
    "$$ w^{HO}_{neu}=w^{HO}+\\Delta w^{HO}$$\n",
    "\n",
    "\n",
    "$$ w^{HO}_{neu}=\\begin{pmatrix}  0,2 \\\\ 0,1 \\\\  0,1 \\end{pmatrix} +\\begin{pmatrix}  0,1031 \\\\ 0,0592 \\\\  0,0567 \\end{pmatrix}=\\begin{pmatrix}  0,3031 \\\\ 0,1592 \\\\  0,1567 \\end{pmatrix} $$\n",
    "\n",
    "#### Zusammenfassung\n",
    "$$\\Delta w^{HO} = \\eta \\cdot HO^{T}\\cdot(t-FO) \\cdot FO\\cdot (1- FO)$$\n",
    "Zur besseren Übersicht fasst man die beiden letzten Faktoren zu einem Korrekturwert Final Delta zusammen:\n",
    "$$FDelta= (t-FO) \\cdot FO\\cdot (1- FO)$$\n",
    "\n",
    "Man kann die Korrektur der Gewichte also interpretieren als Produkt aus Lernrate, Ausgabe der Schicht, Ableitung der Aktivierungsfunktion und Fehler. Das läßt sich auf die anderen Schichten übertragen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gewichtskorrektur aus der Hidden-Schicht heraus  \n",
    "\n",
    "Analog wie oben geht man vor um die Korrektur von w<sup>IH</sup> zu berechnen. Allerdings ist die Berechnung des Fehlers etwas komplizierter.\n",
    "In der Ausgabeschicht war es die Differenz aus Ausgabe und Zielwert:\n",
    "$$FErr = t-FO$$\n",
    "\n",
    "hier wird der Koorekturwert Final Delta mit dem jeweiligen Gewicht gewichtet:\n",
    "\n",
    "$$HErr= w^{HO}\\cdot FDelta$$\n",
    "$$HErr= \\begin{pmatrix}  0,2 & 0,1 &  0,1 \\end{pmatrix} \\cdot 0,1031=\\begin{pmatrix}  0,0206 & 0,0103 &  0,0103 \\end{pmatrix} $$\n",
    "\n",
    "Dieser Fehler wird mit der Ableitung der Aktivierung multipliziert und das ergibt dann HDelta:\n",
    "$$HDelta= HErr \\cdot HO \\cdot (1-HO) $$\n",
    "\n",
    "Diese Vektoren werden elementweise multipliziert:\n",
    "$$HDelta=\\begin{pmatrix}  0,0206 & 0,0103 &  0,0103 \\end{pmatrix} \\cdot \n",
    "\\begin{pmatrix}  0 & 0,2445 &  0,2475 \\end{pmatrix}=\n",
    "\\begin{pmatrix}  0 & 0,0025 &  0,0025 \\end{pmatrix}$$\n",
    "\n",
    "Damit können nun die Korrekturen für die Gewichte berechnet werden:\n",
    "\n",
    "$$\\Delta w^{IH} = \\eta \\cdot IN^{T}\\cdot HDelta$$\n",
    "\n",
    "$$\\Delta w^{IH} = 1 \\cdot \\begin{pmatrix}  1 & 0 &  1 \\end{pmatrix}^{T}\\cdot \n",
    "\\begin{pmatrix}  0 & 0,0025 &  0,0025 \\end{pmatrix}=\n",
    "\\begin{pmatrix}0 & 0,0025 & 0,0025\\\\ 0 & 0 & 0 \\\\0 &0,0025 &0,0025\\end{pmatrix}$$\n",
    "\n",
    "Damit ergeben sich dann als neue Gewichte:\n",
    "\n",
    "$$ w^{IH}= \\begin{pmatrix}0 & 0,2025 & 0,1025\\\\ 0 & -0,2 & 0,1 \\\\0 &0,1025 &0,1025\\end{pmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rechenbeispiel als Excel-Tabelle:\n",
    "hier gelten:\n",
    "$$der = FO\\cdot (1- FO) $$und für die Output-Schicht:\n",
    "\n",
    "$$delta = (t-FO)\\cdot der$$\n",
    "sowie \n",
    "$$\\Delta w^{HO}_{01}= \\eta \\cdot delta \\cdot HO_0$$\n",
    "![Berechnung](bilder/ergebnisseXOR101_neu.png \"Rechnung\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
