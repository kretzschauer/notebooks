{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lernen im mehrschichtigen Netz\n",
    "## Entwurf des Netzes\n",
    "Das folgende Netz soll programmiert werden. Es besitzt zwei Eingabeneuronen plus ein Bias-Neuron in der Eingabeschicht, zwei Neuronen plus ein Bias-Neuron in der verdeckten Schicht und ein Neuron für die Ausgabe. Die Bezeichnungen für die Gewichte sind so, wie sie im folgenden benutzt werden, die Indizes geben an \"wohin\" \"woher\".\n",
    "![Netzaufbau](bilder/NeuronNetz331.png \"Titel\")\n",
    "Mit diesem Netz soll die XOR-Funktion berechnet und gelernt werden.\n",
    "Für die folgenden Beispielrechnungen werden die folgenden Vereinfachungen gemacht:\n",
    "* alle Gewichte werden mit 1 initialisiert\n",
    "* der Lernfaktor beträgt ebenfalls 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Berechnung der Netzeingabe\n",
    "Als Eingabe soll der Vektor (1, 0, 0) dienen, die erste 1 ist vom Bias-Neuron, die beiden anderen repräsentieren x1 und x2.\n",
    "Für den Knoten 1 der versteckten Schicht berechnet sich die Eingabe wie folgt:\n",
    "<span class=\"math\">$$HI_1 = w_{10}\\cdot x_0 + w_{11}\\cdot x_1 + w_{12}\\cdot x_2 = 1 \\cdot 1 +1 \\cdot 0 +1 \\cdot 0 = 1 $$</span> \n",
    "und analog für Knoten 2:\n",
    "$$HI_2 = w_{20}\\cdot x_0 + w_{21}\\cdot x_1 + w_{22}\\cdot x_2 = 1 \\cdot 1 +1 \\cdot 0 +1 \\cdot 0 = 1 $$\n",
    "Das lässt sich sehr elegant als Produkt der Gewichtsmatrix mit dem Eingabevektor beschreiben und implementieren und ergibt HI, die Eingabe in die versteckte Schicht:\n",
    "<span class=\"math\">$$W^{IH}\\cdot IN = HI  $$</span>\n",
    "\n",
    "<math>$$\\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1\\end{pmatrix} \\cdot \n",
    "     \\begin{pmatrix}  1  \\\\ 0  \\\\ 0  \\end{pmatrix} =  \\begin{pmatrix}1 \\\\ 1\\\\1 \\end{pmatrix} $$</math>  \n",
    "    \n",
    "Dabei kann die erste Komponente des Ergebnisvektors als Eingabe in den Bias-Knoten der verdeckten Schicht interpretiert werden und wird ignoriert. In der Implementierung haben die Matrizen aus Gründen der Einheitlichkeit stets diese Dimensionen und auch dabei werden einzelne Ergebnisse ignoriert.\n",
    "\n",
    "## Berechnung der Aktivierung und der Ausgabe\n",
    "Auf die Eingaben der verdeckten Schicht wird nun jeweils die Stufenfunktion angewendet und man erhält HO, die Ausgabe der versteckten Schicht:\n",
    "<math>$$ HO = \\sigma(HI) = \\sigma\\begin{pmatrix}  1  \\\\ 0  \\\\ 0  \\end{pmatrix} =  \\begin{pmatrix}  1  \\\\ 0,73105  \\\\ 0,73105  \\end{pmatrix} $$</math>\n",
    "\n",
    "Mit der Ausgabe der versteckten Schicht berechnet man die Eingabe in das Ausgabeneuron FI (final in).\n",
    "\n",
    "$$W^{HO}\\cdot HO = FI  $$\n",
    "\n",
    "$$\\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} \\cdot \n",
    "     \\begin{pmatrix}  1  \\\\ 0,73105  \\\\ 0,73105  \\end{pmatrix} =  \\begin{pmatrix}2,46211 \\end{pmatrix} $$ \n",
    "\n",
    "Auch auf diese Eingabe wird wieder die Sigmoid-Funktion angewendet, man erhält FO (final out):\n",
    "\n",
    "$$ FO = \\sigma(FI) = \\sigma(2,46211) =  0,92144 $$\n",
    "\n",
    "Damit hat die Eingabe einmal das Netz durchlaufen und verursacht einen Fehler, der dann zur Korrektur der Gewichte benutzt wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "### Gewichtskorrektur aus der Outputschicht heraus\n",
    "Wir nehmen an, dass unser Netz in der Ausgabeschicht einen Fehler E gemacht hat, der sich aus der Differenz des gewünschten Wertes t und der berechneten Ausgabe FO ergibt. \n",
    "\n",
    "$$ E = \\frac {1}{2} (t- FO)^2$$\n",
    "\n",
    "Um den Einfluss der einzelnen Gewichte auf den Fehler zu bestimmen und diese entsprechend zu korrigieren, wird der Fehler nach dem jeweiligen Gewicht abgeleitet. Damit erhält man den Gradienten des Fehlers, das ist ein Vektor, der die Richtung der größten Steigung der Fehlerfunktion angibt. Die Korrektur muss dann also entgegengerichtet zu diesem Vektor erfolgen. \n",
    "\n",
    "$$\\frac {\\delta E}{\\delta w_{ji}}= \\frac {\\delta(\\frac {1}{2} (t- FO)^2)}{\\delta w_{ji}} $$\n",
    "\n",
    "mit der Kettenregel ergibt sich\n",
    "\n",
    "$$\\frac {\\delta E}{\\delta w_{ji}}=-(t-FO) \\cdot  \\frac {\\delta FO}{\\delta w_{ji}} $$\n",
    "\n",
    "mit $$FO = \\sigma(FI)$$ und \n",
    "\n",
    "$$ \\sigma '(FI) = \\sigma(FI)\\cdot (1-\\sigma(FI))$$\n",
    "\n",
    "für die Sigmoid-Funktion bekommt man\n",
    "\n",
    "$$\\frac {\\delta E}{\\delta w_{ji}}=-(t-FO) \\cdot  \\frac {\\delta (\\sigma(FI))}{\\delta w_{ji}} $$\n",
    "\n",
    "$$\\frac {\\delta E}{\\delta w_{ji}}=-(t-FO) \\cdot \\sigma(FI)\\cdot (1- \\sigma(FI))\\cdot \\frac {\\delta (FI)}{\\delta w_{ji}} $$\n",
    "\n",
    "oder \n",
    "\n",
    "$$\\frac {\\delta E}{\\delta w_{ji}}=-(t-FO) \\cdot FO\\cdot (1- FO)\\cdot \\frac {\\delta (FI)}{\\delta w_{ji}} $$\n",
    "\n",
    "Die Eingabe in die Outputschicht FI schließlich hängt direkt von den Gewichten ab, in unserem Beispiel gilt\n",
    "\n",
    "$$FI = w^{HO}_{10}\\cdot HO_0 + w^{HO}_{11}\\cdot HO_1 +w^{HO}_{12}\\cdot HO_2 $$\n",
    "\n",
    "In die Ableitung geht damit nur der Summand ein, in dem das Gewicht, nach dem abgeleitet wird, steht. \n",
    "\n",
    "$$\\frac {\\delta E}{\\delta w_{10}}=-(t-FO) \\cdot FO\\cdot (1- FO)\\cdot \\frac {\\delta (FI)}{\\delta w_{10}} $$\n",
    "\n",
    "$$\\frac {\\delta E}{\\delta w_{10}}=-(t-FO) \\cdot FO\\cdot (1- FO)\\cdot HO_0 $$\n",
    "\n",
    "Für die Gewichtskorrektur wird noch mit (-1) und dem Lernfaktor multipliziert. \n",
    "\n",
    "$$\\Delta w^{HO}_{10}= \\eta \\cdot (t-FO) \\cdot FO\\cdot (1- FO)\\cdot HO_0 \\cdot $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gewichtskorrektur aus der Hidden-Schicht heraus\n",
    "Analog wie oben geht man vor um die Korrektur von w^{IH} zu berechnen. Auch hier muss der Anteil der einzelnen Gewichte am Fehler bestimmt werden. Da man davon ausgeht, dass jeder der Knoten der Hidden-Schicht entsprechend des Gewichts w^{HO} zum Gesamtfehler beigetragen hat, wichtet man den Fehler dieser Knoten mit diesen Gewichten. Es gilt für den i-ten Knoten der Hidden-Schicht\n",
    "\n",
    "$$ delta^{HO}_i = der_i \\cdot w^{HO}_{1i} \\cdot delta^{FO}$$\n",
    "\n",
    "Da es hier insgesamt 9 Gewichtskorrekturen gibt, schreibt sich das besser als Matrizenmultiplikation:\n",
    "\n",
    "$$\\Delta w^{IH}_{ji}= \\eta \\cdot \\begin{pmatrix} -0,0667 \\\\ -0,0131 \\\\ -0,0131 \\end{pmatrix} \\cdot \n",
    "     \\begin{pmatrix}  1  \\\\ 0  \\\\ 0  \\end{pmatrix}^T \n",
    "     = \\eta \\cdot \\begin{pmatrix} -0,0667 \\\\ -0,0131 \\\\ -0,0131 \\end{pmatrix} \\cdot \n",
    "     \\begin{pmatrix}  1  & 0  & 0  \\end{pmatrix}\n",
    "     = \\eta \\cdot \\begin{pmatrix}-0,0667 & 0 & 0\\\\ -0,0131 & 0 & 0 \\\\-0,0131 &0 &0\\end{pmatrix} $$ \n",
    "\n",
    "Allgemein gilt also:\n",
    "\n",
    "$$\\Delta w^{IH}_{ji}= \\eta \\cdot delta^{HO}_j  \\cdot  IN_i^T  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rechenbeispiel als Excel-Tabelle:\n",
    "hier gelten:\n",
    "$$der = FO\\cdot (1- FO) $$und für die Output-Schicht:\n",
    "\n",
    "$$delta = (t-FO)\\cdot der$$\n",
    "sowie \n",
    "$$\\Delta w^{HO}_{10}= \\eta \\cdot delta \\cdot HO_0$$\n",
    "![Berechnung](bilder/excel_vorgerechnet.png \"Rechnung\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
