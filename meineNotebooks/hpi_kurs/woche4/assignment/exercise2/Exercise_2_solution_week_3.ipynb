{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Exercise 2 - solution week 3",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python [conda env:MOOC] *",
      "language": "python",
      "name": "conda-env-MOOC-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntOuL-4wHtZ9",
        "colab_type": "text"
      },
      "source": [
        "# Lösung Übung 2 - Woche 3 und 4\n",
        "\n",
        "Mit dem Wissen aus Woche 3 konnte man noch keine sehr zufriedenstellenden Ergebnisse erzielen. Das Problem hierbei war das schnelle Overfitting auf die Daten, da Dropout noch nicht bekannt war und auch Batch Normalization erst in Woche 4 vorgestellt wurde.\n",
        "\n",
        "Da alle Bilder Hunde zeigen, die sich nicht so einfach voneinander unterscheiden lassen wie die Klassen im ImageNette Datensatz, war hier eine Genauigkeit von 40-50% zu erreichen.\n",
        "\n",
        "Dieses Notebook stellt keine optimale Lösung dar, soll euch aber zeigen, welches grobe Netz gewählt werden kann.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlNuB2X7JSK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PRNIed8V-Rry",
        "colab": {}
      },
      "source": [
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.test.is_gpu_available():\n",
        "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZO6p-aRv99m-",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Dense, Activation, Input, \\\n",
        "  Dropout, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import reciprocal\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "# jupyters magic command\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0de0EjqZ6i7",
        "colab_type": "code",
        "outputId": "7dd686bc-5e8a-4b43-b509-a3b25677e76f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        }
      },
      "source": [
        "!pip install --upgrade deeplearning2020"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deeplearning2020\n",
            "  Downloading https://files.pythonhosted.org/packages/3a/7f/6fee39d7590f4ae20a976131b1920d56a3dee138c208dfcb3959cd8c5275/deeplearning2020-0.4.21.tar.gz\n",
            "Collecting kerasltisubmission>=0.4.9\n",
            "  Using cached https://files.pythonhosted.org/packages/de/56/0b6adef8e6f5d89e9daa68e03d00850509f1553ce6303c0a49d7c619dd26/kerasltisubmission-0.4.9.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from kerasltisubmission>=0.4.9->deeplearning2020) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: progressbar2 in /usr/local/lib/python3.6/dist-packages (from kerasltisubmission>=0.4.9->deeplearning2020) (3.38.0)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from kerasltisubmission>=0.4.9->deeplearning2020) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->kerasltisubmission>=0.4.9->deeplearning2020) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from progressbar2->kerasltisubmission>=0.4.9->deeplearning2020) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->kerasltisubmission>=0.4.9->deeplearning2020) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kerasltisubmission>=0.4.9->deeplearning2020) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->kerasltisubmission>=0.4.9->deeplearning2020) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kerasltisubmission>=0.4.9->deeplearning2020) (3.0.4)\n",
            "Building wheels for collected packages: deeplearning2020, kerasltisubmission\n",
            "  Building wheel for deeplearning2020 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeplearning2020: filename=deeplearning2020-0.4.21-py2.py3-none-any.whl size=8548 sha256=0ea0830d515fd509f6c003f900371260b49d96430d28f264643bbaae8ab7e93f\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/c2/8a/f9f03fc839999f1fe9d5e5a9d2c97cdd5cb8329f61f82ea2c9\n",
            "  Building wheel for kerasltisubmission (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kerasltisubmission: filename=kerasltisubmission-0.4.9-py2.py3-none-any.whl size=8867 sha256=46676efb4bbbe76f1b54684d83c45b469e905af7cf8d5966a5e4826eac478735\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/61/f7/09171376b25408ae21b58e98c9fbf2eb924f676bb77659f983\n",
            "Successfully built deeplearning2020 kerasltisubmission\n",
            "Installing collected packages: kerasltisubmission, deeplearning2020\n",
            "Successfully installed deeplearning2020-0.4.21 kerasltisubmission-0.4.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4f0f8dndDUX",
        "colab_type": "code",
        "outputId": "ad21602d-5bff-4c0b-a1ef-e883ac7f7109",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from deeplearning2020.datasets import ImageWoof\n",
        "train_data, test_data, classes= ImageWoof.load_data()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/fast-ai-imageclas/imagewoof2-320.tgz\n",
            "328294400/328288506 [==============================] - 8s 0us/step\n",
            "/root/.keras/datasets/imagewoof2-320/train\n",
            "Loaded 9025 images\n",
            "/root/.keras/datasets/imagewoof2-320/val\n",
            "Loaded 3929 images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fko1-mNnJ3Ji",
        "colab_type": "text"
      },
      "source": [
        "## Loading and Preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDxi995yRt3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# resize the images to a uniform size\n",
        "def preprocess(image, label):\n",
        "    resized_image = tf.image.resize(image, [300, 300])\n",
        "    return resized_image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-aba_6yC99ni",
        "outputId": "83adf767-7aab-4163-c403-5ae1aecca3f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "\n",
        "\n",
        "batch_size = 32\n",
        "print('shape of training data before preprocessing: ', train_data)\n",
        "train_data = train_data.shuffle(1000)\n",
        "\n",
        "\n",
        "train_data = train_data.map(preprocess) \\\n",
        "  .batch(batch_size).prefetch(1)\n",
        "test_data = test_data.map(preprocess) \\\n",
        "  .batch(batch_size).prefetch(1)\n",
        "print('shape of training data after preprocessing: ', train_data)\n",
        "print('shape of test data after preprocessing: ', test_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of training data before preprocessing:  <ParallelMapDataset shapes: ((None, None, 3), ()), types: (tf.float32, tf.int64)>\n",
            "shape of training data after preprocessing:  <PrefetchDataset shapes: ((None, 300, 300, 3), (None,)), types: (tf.float32, tf.int64)>\n",
            "shape of test data after preprocessing:  <PrefetchDataset shapes: ((None, 300, 300, 3), (None,)), types: (tf.float32, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnHSt3CVwrrP",
        "colab_type": "text"
      },
      "source": [
        "# Architektur des Netzes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hik8RBJpwvvC",
        "colab_type": "code",
        "outputId": "ebb8f672-7249-4fb5-e1a4-9de6f3fe5574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "# model\n",
        "learning_rate=0.001\n",
        "momentum=0.9\n",
        "dense_neurons=500\n",
        "n_filters=512\n",
        "first_kernel_size=(7,7)\n",
        "\n",
        "activation='elu'\n",
        "\n",
        "# input size of images must be 300x300 with RGB color\n",
        "input_layer = Input(shape=(300, 300, 3))\n",
        "\n",
        "# Convolutional Neural Network\n",
        "# It consists of 5 stacked Convolutional Layers with Max Pooling\n",
        "model = Conv2D(\n",
        "    filters=256,\n",
        "    kernel_size=(7,7),\n",
        "    activation=activation\n",
        ")(input_layer)\n",
        "model = MaxPooling2D((2,2))(model)\n",
        "\n",
        "model = Conv2D(\n",
        "    filters = 256, \n",
        "    kernel_size=(3,3), \n",
        "    activation=activation\n",
        ")(model)\n",
        "model = MaxPooling2D((2,2))(model)\n",
        "\n",
        "model = Conv2D(\n",
        "    filters = n_filters, \n",
        "    kernel_size=(3,3), \n",
        "    activation=activation\n",
        ")(model)\n",
        "model = MaxPooling2D((2,2))(model)\n",
        "\n",
        "model = Conv2D(\n",
        "    filters = n_filters, \n",
        "    kernel_size=(3,3), \n",
        "    activation=activation\n",
        ")(model)\n",
        "model = MaxPooling2D((2,2))(model)\n",
        "\n",
        "model = Conv2D(filters = n_filters, \n",
        "  kernel_size=(3,3), \n",
        "  activation=activation, \n",
        "  padding='same'\n",
        ")(model)\n",
        "model = MaxPooling2D((2,2))(model)\n",
        "\n",
        "model = Conv2D(filters = n_filters, \n",
        "  kernel_size=(3,3), \n",
        "  activation=activation, \n",
        "  padding='same'\n",
        ")(model)\n",
        "model = MaxPooling2D((2,2))(model)\n",
        "\n",
        "model = Conv2D(filters = n_filters, \n",
        "  kernel_size=(3,3), \n",
        "  activation=activation, \n",
        "  padding='same'\n",
        ")(model)\n",
        "\n",
        "# Fully-Connected-Classifier\n",
        "model = Flatten()(model)\n",
        "model = Dense(\n",
        "    dense_neurons,\n",
        "    activation=activation\n",
        ")(model)\n",
        "\n",
        "model = Dense(\n",
        "    dense_neurons / 2,\n",
        "    activation='tanh'\n",
        ")(model)\n",
        "\n",
        "# Output Layer\n",
        "output = Dense(10, activation=\"softmax\")(model)\n",
        "\n",
        "model = Model(input_layer, output)\n",
        "\n",
        "# Compiling model\n",
        "optimizer = keras.optimizers.SGD(lr=learning_rate, momentum=momentum)\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=optimizer,\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 300, 300, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 294, 294, 256)     37888     \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 147, 147, 256)     0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 145, 145, 256)     590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 72, 72, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 70, 70, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 35, 35, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 33, 33, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 500)               4096500   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 250)               125250    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                2510      \n",
            "=================================================================\n",
            "Total params: 15,471,620\n",
            "Trainable params: 15,471,620\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9J5rOK-w2qA",
        "colab_type": "code",
        "outputId": "20a1697f-4e23-4a2f-fe01-e5ed0009be52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    epochs=10,\n",
        "    validation_data = test_data\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "283/283 [==============================] - 123s 436ms/step - loss: 0.8194 - accuracy: 0.7309 - val_loss: 1.9239 - val_accuracy: 0.4100\n",
            "Epoch 2/10\n",
            "283/283 [==============================] - 123s 436ms/step - loss: 0.6151 - accuracy: 0.8079 - val_loss: 2.0038 - val_accuracy: 0.4123\n",
            "Epoch 3/10\n",
            "283/283 [==============================] - 123s 436ms/step - loss: 0.2568 - accuracy: 0.9378 - val_loss: 2.2265 - val_accuracy: 0.4065\n",
            "Epoch 4/10\n",
            "283/283 [==============================] - 123s 435ms/step - loss: 0.1921 - accuracy: 0.9591 - val_loss: 2.3152 - val_accuracy: 0.4093\n",
            "Epoch 5/10\n",
            "283/283 [==============================] - 123s 436ms/step - loss: 0.0354 - accuracy: 0.9981 - val_loss: 2.3080 - val_accuracy: 0.4291\n",
            "Epoch 6/10\n",
            "283/283 [==============================] - 123s 436ms/step - loss: 0.0127 - accuracy: 0.9998 - val_loss: 2.3591 - val_accuracy: 0.4383\n",
            "Epoch 7/10\n",
            "283/283 [==============================] - 123s 436ms/step - loss: 0.0079 - accuracy: 0.9999 - val_loss: 2.4098 - val_accuracy: 0.4378\n",
            "Epoch 8/10\n",
            "283/283 [==============================] - 123s 436ms/step - loss: 0.0063 - accuracy: 0.9999 - val_loss: 2.4507 - val_accuracy: 0.4421\n",
            "Epoch 9/10\n",
            "283/283 [==============================] - 123s 436ms/step - loss: 0.0051 - accuracy: 0.9999 - val_loss: 2.4821 - val_accuracy: 0.4434\n",
            "Epoch 10/10\n",
            "283/283 [==============================] - 123s 436ms/step - loss: 0.0043 - accuracy: 0.9999 - val_loss: 2.5138 - val_accuracy: 0.4396\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}