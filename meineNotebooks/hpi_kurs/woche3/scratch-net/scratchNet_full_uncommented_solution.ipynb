{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scratchNet-full-uncommented-solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52hHPx_BQQZ7",
        "colab_type": "text"
      },
      "source": [
        "# ScratchNet\n",
        "Ein einfaches künstliches neuronales Netz mit einer von `keras` inspirierten API.\n",
        "Hinweis: Das Netz wurde ausschließlich für Lernzwecke verfasst."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOixRSglQDcm",
        "colab_type": "code",
        "outputId": "04424eb6-87d6-4306-cdb5-c7c4f733a293",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import abc\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "from tqdm import tqdm, trange"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6c8fBcRQX3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DifferentiableFunction(abc.ABC):\n",
        "    def derivative(self, net_input):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, net_input):\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYf4TAzdQall",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sigmoid(DifferentiableFunction):\n",
        "    def derivative(self, net_input):\n",
        "        return self(net_input) * (1 - self(net_input))\n",
        "\n",
        "    def __call__(self, net_input):\n",
        "        return 1 / (1 + np.exp(-net_input))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX6Hw7hAQduw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SquaredError(DifferentiableFunction):\n",
        "    def derivative(self, target, actual):\n",
        "        return actual - target\n",
        "\n",
        "    def __call__(self, target, actual):\n",
        "        return 0.5 * np.sum((target - actual) ** 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gczgOtCWQh0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DenseLayer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        neuron_count,\n",
        "        depth=None,\n",
        "        activation=None,\n",
        "        biases=None,\n",
        "        weights=None,\n",
        "        prev_layer=None,\n",
        "        next_layer=None,\n",
        "    ):\n",
        "        self.depth = depth\n",
        "        self.next_layer = next_layer\n",
        "        self.prev_layer = prev_layer\n",
        "\n",
        "        self.neuron_count = neuron_count\n",
        "        self.activation_func = activation or Sigmoid()\n",
        "\n",
        "        self.weights = weights\n",
        "        self.biases = biases\n",
        "\n",
        "    def prepare_inputs(self, images, labels=None):\n",
        "        return images if labels is None else images, labels\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        if self.weights is None:\n",
        "            self.weights = np.random.randn(\n",
        "                self.neuron_count, self.prev_layer.neuron_count\n",
        "            )\n",
        "        if self.biases is None:\n",
        "            self.biases = np.random.randn(self.neuron_count, 1)\n",
        "\n",
        "    def compute_cost_gradients(self, label_vec, cost_func):\n",
        "        cost_gradients = cost_func.derivative(\n",
        "            self.activation_vec, label_vec\n",
        "        ) * self.activation_func.derivative(self.layer_inputs)\n",
        "        self._update_layer_gradients(cost_gradients)\n",
        "        return cost_gradients\n",
        "\n",
        "    def feed_backwards(self, prev_input_gradients):\n",
        "        new_input_gradients = np.dot(\n",
        "            self.next_layer.weights.transpose(), prev_input_gradients\n",
        "        ) * self.activation_func.derivative(self.layer_inputs)\n",
        "        self._update_layer_gradients(new_input_gradients)\n",
        "        return new_input_gradients\n",
        "\n",
        "    def _update_layer_gradients(self, input_gradients):\n",
        "        self.bias_gradients = input_gradients\n",
        "        self.weight_gradients = np.dot(\n",
        "            input_gradients, self.prev_layer.activation_vec.transpose()\n",
        "        )\n",
        "\n",
        "    def feed_forward_layer(self, input_activations):\n",
        "        self.layer_inputs = np.dot(\n",
        "            self.weights, input_activations) + self.biases\n",
        "        self.activation_vec = self.activation_func(self.layer_inputs)\n",
        "        return self.activation_vec\n",
        "\n",
        "    def inspect(self):\n",
        "        print(f\"--------- Layer L={self.depth} ---------\")\n",
        "        print(f\"  # Neuronen: {self.neuron_count}\")\n",
        "        for n in range(self.neuron_count):\n",
        "            print(f\"    Neuron {n}\")\n",
        "            if self.prev_layer:\n",
        "                for w in self.weights[n]:\n",
        "                    print(f\"      Weight: {w}\")\n",
        "                print(f\"      Bias: {self.biases[n][0]}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKqSvjCgQjHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FlattenLayer(DenseLayer):\n",
        "    def __init__(self, input_shape):\n",
        "        total_input_neurons = 1\n",
        "        for dim in input_shape:\n",
        "            total_input_neurons *= dim\n",
        "        super().__init__(neuron_count=total_input_neurons)\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        pass\n",
        "\n",
        "    def feed_forward_layer(self, input_activations):\n",
        "        self.activation_vec = input_activations\n",
        "        return input_activations\n",
        "\n",
        "    def prepare_inputs(self, images, labels=None):\n",
        "        flattened_images = images.reshape(\n",
        "            images.shape[0], self.neuron_count, 1)\n",
        "        if labels is not None:\n",
        "            labels = labels.reshape(labels.shape[0], -1, 1)\n",
        "            return flattened_images, labels\n",
        "        return flattened_images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vB7U79cQsSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ScratchNet:\n",
        "    def __init__(self, layers):\n",
        "        self.learning_rate = 0.5\n",
        "        self.cost_func = SquaredError()\n",
        "        self.layers = layers\n",
        "        for index, layer in enumerate(self.layers):\n",
        "            layer.prev_layer = self.layers[index - 1] if index > 0 else None\n",
        "            layer.next_layer = (\n",
        "                self.layers[index + 1] if index +\n",
        "                1 < len(self.layers) else None\n",
        "            )\n",
        "            layer.depth = index\n",
        "            layer.initialize_parameters()\n",
        "\n",
        "    def _calculate_loss(self, input_samples):\n",
        "        total_error = 0.0\n",
        "        for sample in input_samples:\n",
        "            image, label_vec = sample\n",
        "            output_activations = self._feed_forward(image)\n",
        "            total_error += self.cost_func(label_vec, output_activations)\n",
        "        return total_error / len(input_samples)\n",
        "\n",
        "    def _calculate_accuracy(self, input_samples):\n",
        "        results = [\n",
        "            (np.argmax(self._feed_forward(image)), np.argmax(expected_label))\n",
        "            for image, expected_label in input_samples\n",
        "        ]\n",
        "        num_correct = sum(int(x == y) for (x, y) in results)\n",
        "        return num_correct / len(input_samples)\n",
        "\n",
        "    def _feed_forward(self, input_sample):\n",
        "        for layer in self.layers:\n",
        "            input_sample = layer.feed_forward_layer(input_sample)\n",
        "        return input_sample\n",
        "\n",
        "    def _update_parameters(self, input_samples):\n",
        "        weight_gradients = [np.zeros(layer.weights.shape)\n",
        "                            for layer in self.layers[1:]]\n",
        "        bias_gradients = [np.zeros(layer.biases.shape)\n",
        "                          for layer in self.layers[1:]]\n",
        "\n",
        "        for sample in input_samples:\n",
        "            sample_weight_gradients, sample_bias_gradients = self._backpropagate(\n",
        "                sample)\n",
        "            weight_gradients = np.add(\n",
        "                weight_gradients, sample_weight_gradients)\n",
        "            bias_gradients = np.add(bias_gradients, sample_bias_gradients)\n",
        "\n",
        "        for layer, layer_weight_gradients, layer_bias_gradients in zip(\n",
        "            self.layers[1:], weight_gradients, bias_gradients\n",
        "        ):\n",
        "            layer.weights += (\n",
        "                self.learning_rate *\n",
        "                layer_weight_gradients / len(input_samples)\n",
        "            )\n",
        "            layer.biases += (\n",
        "                self.learning_rate * layer_bias_gradients / len(input_samples)\n",
        "            )\n",
        "\n",
        "    def _backpropagate(self, training_sample):\n",
        "        train_input, train_output = training_sample\n",
        "        self._feed_forward(train_input)\n",
        "        gradients = self.layers[-1].compute_cost_gradients(\n",
        "            train_output, cost_func=self.cost_func\n",
        "        )\n",
        "\n",
        "        for layer in reversed(self.layers[1:-1]):\n",
        "            gradients = layer.feed_backwards(gradients)\n",
        "\n",
        "        weight_gradients = [\n",
        "            layer.weight_gradients for layer in self.layers[1:]]\n",
        "        bias_gradients = [layer.bias_gradients for layer in self.layers[1:]]\n",
        "        return weight_gradients, bias_gradients\n",
        "\n",
        "    def _stochastic_gradient_descent(\n",
        "        self, training_data, epochs=1, batch_size=1, avg_lookbehind=None\n",
        "    ):\n",
        "        losses, accuracies = list(), list()\n",
        "        training_set_size = len(training_data)\n",
        "\n",
        "        avg_lookbehind = avg_lookbehind or int(\n",
        "            0.10 * training_set_size / batch_size)\n",
        "        running_loss, running_acc = [], []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "\n",
        "            with tqdm(total=training_set_size) as progress:\n",
        "                for t in range(0, training_set_size, batch_size):\n",
        "                    batch = training_data[t: t + batch_size]\n",
        "\n",
        "                    self._update_parameters(batch)\n",
        "                    loss, accuracy = (\n",
        "                        self._calculate_loss(batch),\n",
        "                        self._calculate_accuracy(batch),\n",
        "                    )\n",
        "\n",
        "                    running_loss = ([loss] + running_loss)[:avg_lookbehind]\n",
        "                    running_acc = ([accuracy] + running_acc)[:avg_lookbehind]\n",
        "                    running_loss_avg = np.average(running_loss)\n",
        "                    running_acc_avg = np.average(running_acc)\n",
        "                    losses.append(running_loss_avg)\n",
        "                    accuracies.append(running_acc_avg)\n",
        "\n",
        "                    progress.set_description(f\"Epoch {epoch+1}\")\n",
        "                    progress.set_postfix(\n",
        "                        loss=\"{0:.3f}\".format(running_loss_avg),\n",
        "                        accuracy=\"{0:.2f}\".format(running_acc_avg),\n",
        "                    )\n",
        "\n",
        "                    progress.update(len(batch))\n",
        "        return losses, accuracies\n",
        "\n",
        "    def fit(self, train_images, train_labels, epochs=1, batch_size=1):\n",
        "        train_images, train_labels = self.layers[0].prepare_inputs(\n",
        "            train_images, train_labels\n",
        "        )\n",
        "        training_data = list(zip(train_images, train_labels))\n",
        "        losses, accuracies = self._stochastic_gradient_descent(\n",
        "            training_data, epochs=epochs, batch_size=batch_size\n",
        "        )\n",
        "        return losses, accuracies\n",
        "\n",
        "    def predict(self, model_inputs):\n",
        "        model_inputs = self.layers[0].prepare_inputs(model_inputs)\n",
        "        predicted = np.zeros(\n",
        "            (model_inputs.shape[0], self.layers[-1].neuron_count, 1))\n",
        "        for i, model_input in enumerate(model_inputs):\n",
        "            predicted[i] = self._feed_forward(model_input)\n",
        "        return predicted\n",
        "\n",
        "    def evaluate(self, validation_images, validation_labels):\n",
        "        validation_images, validation_labels = self.layers[0].prepare_inputs(\n",
        "            validation_images, validation_labels\n",
        "        )\n",
        "        validation_data = list(zip(validation_images, validation_labels))\n",
        "        return (\n",
        "            self._calculate_loss(validation_data),\n",
        "            self._calculate_accuracy(validation_data),\n",
        "        )\n",
        "\n",
        "    def compile(self, learning_rate=None, loss=None):\n",
        "        self.learning_rate = learning_rate or self.learning_rate\n",
        "        self.cost_func = loss or self.cost_func\n",
        "\n",
        "    def inspect(self):\n",
        "        print(f\"--------- {self.__class__.__name__} ---------\")\n",
        "        print(f\"  # Inputs: {self.layers[0].neuron_count}\")\n",
        "        for layer in self.layers:\n",
        "            layer.inspect()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}