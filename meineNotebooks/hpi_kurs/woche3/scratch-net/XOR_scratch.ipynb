{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "52hHPx_BQQZ7"
   },
   "source": [
    "# ScratchNet\n",
    "Ein einfaches künstliches neuronales Netz mit einer von `keras` inspirierten API.\n",
    "Hinweis: Das Netz wurde ausschließlich für Lernzwecke verfasst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "colab_type": "code",
    "id": "yzSm4SyFRe4Q",
    "outputId": "f1c5f56b-03f2-4e2a-a726-bafd6b7ef193"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: deeplearning2020 in c:\\users\\jan\\appdata\\roaming\\python\\python38\\site-packages (0.4.21)\n",
      "Requirement already satisfied: kerasltisubmission>=0.4.9 in c:\\users\\jan\\appdata\\roaming\\python\\python38\\site-packages (from deeplearning2020) (0.4.9)\n",
      "Requirement already satisfied: progressbar2 in c:\\program files\\python38\\lib\\site-packages (from kerasltisubmission>=0.4.9->deeplearning2020) (3.50.1)\n",
      "Requirement already satisfied: numpy in c:\\program files\\python38\\lib\\site-packages (from kerasltisubmission>=0.4.9->deeplearning2020) (1.19.5)\n",
      "Requirement already satisfied: requests in c:\\program files\\python38\\lib\\site-packages (from kerasltisubmission>=0.4.9->deeplearning2020) (2.23.0)\n",
      "Requirement already satisfied: python-utils>=2.3.0 in c:\\program files\\python38\\lib\\site-packages (from progressbar2->kerasltisubmission>=0.4.9->deeplearning2020) (2.4.0)\n",
      "Requirement already satisfied: six in c:\\program files\\python38\\lib\\site-packages (from progressbar2->kerasltisubmission>=0.4.9->deeplearning2020) (1.15.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\program files\\python38\\lib\\site-packages (from requests->kerasltisubmission>=0.4.9->deeplearning2020) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\program files\\python38\\lib\\site-packages (from requests->kerasltisubmission>=0.4.9->deeplearning2020) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\program files\\python38\\lib\\site-packages (from requests->kerasltisubmission>=0.4.9->deeplearning2020) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python38\\lib\\site-packages (from requests->kerasltisubmission>=0.4.9->deeplearning2020) (2019.11.28)\n"
     ]
    }
   ],
   "source": [
    "#%tensorflow_version 2.x\n",
    "!pip install --upgrade deeplearning2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JOixRSglQDcm"
   },
   "outputs": [],
   "source": [
    "import abc\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "#from deeplearning2020 import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6c8fBcRQX3e"
   },
   "outputs": [],
   "source": [
    "class DifferentiableFunction(abc.ABC):\n",
    "    \"\"\" Abstrakte Klasse einer differenzierbaren Funktion\n",
    "        Die Implementierung der Methoden erfolgt durch Spezialisierung.\n",
    "    \"\"\"\n",
    "\n",
    "    def derivative(self, net_input):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, net_input):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JYf4TAzdQall"
   },
   "outputs": [],
   "source": [
    "class Sigmoid(DifferentiableFunction):\n",
    "    \"\"\" Sigmoid Aktivierungsfunktion\n",
    "        Stetige und differenzierbare Funktion, dessen Graph der Treppenfunktion ähnelt.\n",
    "    \"\"\"\n",
    "\n",
    "    def derivative(self, net_input):\n",
    "        return self(net_input) * (1 - self(net_input))\n",
    "\n",
    "    def __call__(self, net_input):\n",
    "        return 1 / (1 + np.exp(-net_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YX6Hw7hAQduw"
   },
   "outputs": [],
   "source": [
    "class SquaredError(DifferentiableFunction):\n",
    "    \"\"\" Quadratische Fehlerfunktion\n",
    "        Durch das Quadrieren wird sichergestellt, dass der Fehler nicht negativ wird\n",
    "        und höhere Differenzen stärker ins Gewicht fallen.\n",
    "    \"\"\"\n",
    "\n",
    "    def derivative(self, target, actual):\n",
    "        return actual - target\n",
    "\n",
    "    def __call__(self, target, actual):\n",
    "        return 0.5 * np.sum((target - actual) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gczgOtCWQh0O"
   },
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    \"\"\" DenseLayer\n",
    "\n",
    "        Implementiert einen Layer, dessen Neuronen mit jeweils allen Neuronen aus der Schicht davor verbunden sind. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        neuron_count,\n",
    "        depth=None,\n",
    "        activation=None,\n",
    "        biases=None,\n",
    "        weights=None,\n",
    "        prev_layer=None,\n",
    "        next_layer=None,\n",
    "    ):\n",
    "        \"\"\" Initialisiert DenseLayer L \"\"\"\n",
    "        self.depth = depth\n",
    "        self.next_layer = next_layer\n",
    "        self.prev_layer = prev_layer\n",
    "\n",
    "        self.neuron_count = neuron_count\n",
    "        self.activation_func = activation or Sigmoid()\n",
    "\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "\n",
    "    def prepare_inputs(self, images, labels=None):\n",
    "        \"\"\" Bereitet die Eingabedaten für den DenseLayer L vor\n",
    "\n",
    "            Ein `DenseLayer` benötigt keine besondere Vorbereitung der Eingabe, anders als zum Beispiel der `FlattenLayer`.\n",
    "        \"\"\"\n",
    "        return images if labels is None else images, labels\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\" Initialisiert die Netzparameter für DenseLayer L\n",
    "\n",
    "            Die Parameter der Layer werden zufällig gewählt.\n",
    "            Die Gewichte von L werden durch eine Matrix mit `shape=(n_L, n_P)` repräsentiert,\n",
    "            wobei `n_L` die Anzahl an Neuronen des aktuellen Layer `L` und `n_P` die Anzahl an Neuronen im vorigen Layer `P = L - 1` ist.\n",
    "            Der Eintrag `(i,j)` enspricht dem Gewicht, mit dem die Neuronen `i` (Layer `L`) und `j` (Layer `P = L - 1`) miteinander verbunden sind.\n",
    "\n",
    "            Die Biases von L werden durch eine Matrix mit `shape=(n_L, 1)` repräsentiert,\n",
    "            wobei `n_L` die Anzahl an Neuronen des aktuellen Layer `L` ist.\n",
    "            Der Eintrag `(i,1)` enspricht dem Bias, der zu dem Neuron `i` im aktuellen Layer `L` addiert / subtrahiert wird.\n",
    "            Da die Subtraktion des Biases `b_i = biases[i,1]` der Addition des negierten Bias `-b_i` entspricht, spielt dies für das Netz keine Rolle.\n",
    "        \"\"\"\n",
    "        # `np.random.randn(d0, d1, ..., dn)` initialisiert eine Matrix mit shape (d0, d1, ..., dn) mit Werten aus der Standard Normalverteilung\n",
    "        if self.weights is None:\n",
    "            self.weights = np.random.randn(\n",
    "                self.neuron_count, self.prev_layer.neuron_count\n",
    "            )\n",
    "        if self.biases is None:\n",
    "            self.biases = np.random.randn(self.neuron_count, 1)\n",
    "\n",
    "    def compute_cost_gradients(self, label_vec, cost_func):\n",
    "        \"\"\" Bestimmt die Gradienten ∇aLC der Kostenfunktion C in Abhängigkeit der \n",
    "            Aktivierungen in DenseLayer L\n",
    "\n",
    "            Dieser lässt sich sehr einfach ohne Abhängigkeiten berechnen:\n",
    "            Dazu wird der Gradientenvektor der Kostenfunktion für ein Trainingssample mit dem\n",
    "            Gradientenvektor der Aktivierungen `self.activation_vec` in Layer `L` multipliziert,\n",
    "            da die Aktivierungen als Eingabe der Kostenfunktion diese direkt beeinflussen.\n",
    "            Diese Gradienten bilden dann die Startwerte für den Backpropagation Schritt;\n",
    "            `compute_cost_gradients` wird demnach nur auf dem Output Layer aufgerufen.\n",
    "            Anschließend werden die Gradienten der Weights und Biases des Layers in \n",
    "            `_update_layer_gradients` aktualisiert.\n",
    "        \"\"\"\n",
    "        cost_gradients = cost_func.derivative(\n",
    "            self.activation_vec, label_vec\n",
    "        ) * self.activation_func.derivative(self.layer_inputs)\n",
    "        self._update_layer_gradients(cost_gradients)\n",
    "        return cost_gradients\n",
    "\n",
    "    def feed_backwards(self, prev_input_gradients):\n",
    "        \"\"\" Feed-backward Schritt für DenseLayer L zur Berechnung des neuen Eingabegradienten δL\n",
    "\n",
    "            Im Backpropagation Schritt wird der Fehler des Netzes rückwärts durch das Netz\n",
    "            propagiert. Da die Werte rückwärts propagiert werden, ist der vorherige Layer \n",
    "            `self.next_layer` (L+1). Es wird das Produkt aus der transponierten Gewichtsmatrix \n",
    "            (WL)T des vorherigen Layer mit den Gradienten δL+1 aus dem letzen `feed_backwards` Schritt\n",
    "            gebildet und mit dem Gradientenvektor der Aktivierungen in Layer L multipliziert.\n",
    "            Die Gewichtsmatrix hat eine `shape=(n_L+1, n_L)`, und wird transponiert, da wir nun rückwärts\n",
    "            durch das Netz propagieren und mit den Gradienten aus dem vorigen Layer L+1\n",
    "            mit `shape=(n_L+1, 1)` multiplizieren wollen. Durch das Transponieren erhalten wir \n",
    "            eine `shape=(n_L, n_L+1)` so, dass das Ergebnis mit dem Gradientenvektor der Aktivierungen\n",
    "            in Layer L mit `shape=(n_L+1, 1)` multiplizieren können.\n",
    "\n",
    "            Als Errinnerung: `self.layer_inputs` ist der Vektor, dessen Einträge jeweils den \n",
    "            summierten Eingaben der Neuronen in Layer L entsprechen.\n",
    "            Anschließend werden die Gradienten der Weights und Biases des Layers in `_update_layer_gradients`\n",
    "            aktualsiert.\n",
    "        \"\"\"\n",
    "        new_input_gradients = np.dot(\n",
    "            self.next_layer.weights.transpose(), prev_input_gradients\n",
    "        ) * self.activation_func.derivative(self.layer_inputs)\n",
    "        self._update_layer_gradients(new_input_gradients)\n",
    "        return new_input_gradients\n",
    "\n",
    "    def _update_layer_gradients(self, input_gradients):\n",
    "        \"\"\" Aktualisiert die Gradienten der Weights und Biases für DenseLayer L\n",
    "\n",
    "            Die Gradienten der Biases entsprechen den `input_gradients` in diesem Layer,\n",
    "            da die biases direkt proportionalen Einfluss auf Aktivierungen der Neuronen in Layer L haben.\n",
    "\n",
    "            Die Gradienten der Gewichte an den Kanten zu dem vorigen Layer entsprechen dem Produkt\n",
    "            der `input_gradients` mit dem transponierten Gradientenvektor der Aktivierungen des \n",
    "            vorigen Layers L-1. Ebenso wie in `feed_backwards` ist hier die Reihenfolge der Multiplikation vertauscht\n",
    "            und die Transposition wird benötigt, damit die Matrizen die richtige Form haben.\n",
    "        \"\"\"\n",
    "        self.bias_gradients = input_gradients\n",
    "        self.weight_gradients = np.dot(\n",
    "            input_gradients, self.prev_layer.activation_vec.transpose()\n",
    "        )\n",
    "\n",
    "    def feed_forward_layer(self, input_activations):\n",
    "        \"\"\" Feedforward Schritt für DenseLayer L\n",
    "\n",
    "            Als Eingabe werden die Aktivierungen aus dem vorherigen Layer übergeben.\n",
    "            Zunächst werden die summierten Eingaben der jeweiligen Neuronen in Layer L berechnet.\n",
    "            Da es sich um einen `DenseLayer` handelt wird die Gewichtsmatrix der Kanten zwischen den \n",
    "            Layern P und L (`shape=(n_L, n_P)`) mit den `input_activations` (`shape=(n_P, 1)` mit P=L-1)\n",
    "            aller Neuronen aus dem vorherigen Layer multipliziert.\n",
    "            Auf die summierten Eingaben der Neuronen (`shape=(n_L, 1)`) werden dann die \n",
    "            Biases des Layer L (`shape=(n_L, 1)`) addiert und as Ergebnis in `self.layer_inputs` zwischengespeichert.\n",
    "            Abschließend werden auch die Aktivierungen der Neuronen in Layer L berechnet, indem die Aktivierungsfunktion\n",
    "            auf die summierten Eingaben der Neuronen angewandt wird.\n",
    "        \"\"\"\n",
    "        self.layer_inputs = np.dot(\n",
    "            self.weights, input_activations) + self.biases\n",
    "        self.activation_vec = self.activation_func(self.layer_inputs)\n",
    "        return self.activation_vec\n",
    "\n",
    "    def inspect(self):\n",
    "        \"\"\" Inspiziert DenseLayer L \"\"\"\n",
    "        print(f\"--------- Layer L={self.depth} ---------\")\n",
    "        print(f\"  # Neuronen: {self.neuron_count}\")\n",
    "        for n in range(self.neuron_count):\n",
    "            print(f\"    Neuron {n}\")\n",
    "            if self.prev_layer:\n",
    "                for w in self.weights[n]:\n",
    "                    print(f\"      Weight: {w}\")\n",
    "                print(f\"      Bias: {self.biases[n][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lKqSvjCgQjHS"
   },
   "outputs": [],
   "source": [
    "class FlattenLayer(DenseLayer):\n",
    "    def __init__(self, input_shape):\n",
    "        \"\"\" Initialisiert den FlattenLayer\n",
    "\n",
    "            Die Unterscheidung zu einem `DenseLayer` ist in unserer Implementierung fast nur semantisch.\n",
    "            Lediglich die Anzahl der Neuronen wird durch die `input_shape` vorgegeben und die Eingaben des\n",
    "            Layers werden vorher mit `reshape` auf den Input Layer angepasst.  \n",
    "        \"\"\"\n",
    "        total_input_neurons = 1\n",
    "        # Beispiele:\n",
    "        # (28,28)   wird zu 28*28=784\n",
    "        # (28,28,1) wird zu 28*28*1=784\n",
    "        # (4,4,2)   wird zu 4*4*2=32\n",
    "        for dim in input_shape:\n",
    "            total_input_neurons *= dim\n",
    "        super().__init__(neuron_count=total_input_neurons)\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\" Initialisiert die Netzparameter für FlattenLayer L\n",
    "\n",
    "            Für einen `FlattenLayer` ist keine Initialisierung erforderlich, da dieser nur als\n",
    "            Input Layer verwendet wird und die Aktivierungen des `FlattenLayer` durch die Werte\n",
    "            des aktuellen Trainingsbeispiels (siehe `feed_forward_layer`) gegeben sind.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def feed_forward_layer(self, input_activations):\n",
    "        \"\"\" Führt den Feedforward Schritt für FlattenLayer L durch\n",
    "\n",
    "            Für einen `FlattenLayer`, der als Input Layer dient, sind `input_activations` die Werte\n",
    "            des aktuellen Trainingsbeispiels, welche unverändert als Aktivierungen der Neuronen übernommen werden.\n",
    "        \"\"\"\n",
    "        self.activation_vec = input_activations\n",
    "        return input_activations\n",
    "\n",
    "    def prepare_inputs(self, images, labels=None):\n",
    "        \"\"\" Bereitet die Eingabedaten für FlattenLayer L vor\n",
    "\n",
    "            Der `FlattenLayer` nutzt diese Methode um die Eingabematrizen mit `reshape` in eine flache Form zu bringen.\n",
    "            Die Shape der Einträge dieser flachen Matrix ist durch `self.neuron_count` vorgegeben.\n",
    "            Zudem werden die Eingabematrizen wie Labels (falls vorhanden) um eine Dimension erweitert,\n",
    "            um auf jeden Fall transponiert werden zu können.\n",
    "        \"\"\"\n",
    "        flattened_images = images.reshape(\n",
    "            images.shape[0], self.neuron_count, 1)\n",
    "        if labels is not None:\n",
    "            labels = labels.reshape(labels.shape[0], -1, 1)\n",
    "            return flattened_images, labels\n",
    "        return flattened_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8vB7U79cQsSI"
   },
   "outputs": [],
   "source": [
    "class ScratchNet:\n",
    "    def __init__(self, layers):\n",
    "        \"\"\" Initialisert das Netz\n",
    "\n",
    "            Die Layer des Netzes werden miteinander verkettet,\n",
    "            sodass jeder Layer L seinen Index und eine Referenz auf seine Nachbarn erhält.\n",
    "            Anschließend können die Parameter der Layer initialisiert werden.\n",
    "        \"\"\"\n",
    "        self.learning_rate = 0.5\n",
    "        self.cost_func = SquaredError()\n",
    "        self.layers = layers\n",
    "        for index, layer in enumerate(self.layers):\n",
    "            layer.prev_layer = self.layers[index - 1] if index > 0 else None\n",
    "            layer.next_layer = (\n",
    "                self.layers[index + 1] if index +\n",
    "                1 < len(self.layers) else None\n",
    "            )\n",
    "            layer.depth = index\n",
    "            layer.initialize_parameters()\n",
    "\n",
    "    def _calculate_loss(self, input_samples):\n",
    "        \"\"\" Berechnet den Fehler des Netzes auf den übergebenen `input_samples`\n",
    "\n",
    "            Der Fehler ist dabei definiert als der Durchschnitt über den Fehlern der \n",
    "            `input_samples`. Um den Fehler eines Samples zu berechnen, wird dessen Ausgabe\n",
    "            im Feedforward Schritt berechnet und mit dem erwarteten Label als Argument der\n",
    "            Kostenfunktion `cost_func` übergeben.\n",
    "        \"\"\"\n",
    "        total_error = 0.0\n",
    "        for sample in input_samples:\n",
    "            image, label_vec = sample\n",
    "            output_activations = self._feed_forward(image)\n",
    "            total_error += self.cost_func(label_vec, output_activations)\n",
    "        return total_error / len(input_samples)\n",
    "\n",
    "    def _calculate_accuracy(self, input_samples):\n",
    "        \"\"\" Berechnet die Accuracy des Netzes auf den übergebenen `input_samples`\n",
    "\n",
    "            Für jedes Sample wird im Feedforward Schritt die Ausgabe des Netzes berechnet und\n",
    "            mithilfe der `np.argmax` Funktion, welche für eine Matrix den Index des Eintrags\n",
    "            mit dem höchsten Wert berechnet, mit dem erwarteten Label verglichen.\n",
    "            Dabei haben sowohl die Aktivierungsmatrix im Output Layer als auch die Matrix des \n",
    "            erwarteten Label eine `shape=(nL,1)` wobei `nL` die Zahl der Neuronen im Output Layer L sind.\n",
    "            Die Accuracy ist dann definiert als der Quotient `num_correct/len(input_samples)`.\n",
    "        \"\"\"\n",
    "        results = [\n",
    "            (np.argmax(self._feed_forward(image)), np.argmax(expected_label))\n",
    "            for image, expected_label in input_samples\n",
    "        ]\n",
    "        num_correct = sum(int(x == y) for (x, y) in results)\n",
    "        return num_correct / len(input_samples)\n",
    "\n",
    "    def _feed_forward(self, input_sample):\n",
    "        \"\"\" Feedforward eines Samples durch das Netz \"\"\"\n",
    "        for layer in self.layers:\n",
    "            # `input_sample` wird mit jedem Layer überschrieben\n",
    "            # Jeder Layer speichert dabei die Aktivierungen seiner Neuronen in `self.activation_vec`\n",
    "            input_sample = layer.feed_forward_layer(input_sample)\n",
    "        return input_sample\n",
    "\n",
    "    def _update_parameters(self, input_samples):\n",
    "        \"\"\" Aktualisiert die Parameter (Weights und Biases) des Netzes\n",
    "\n",
    "            Für jedes Input Sample werden mit Backpropagation die Gradienten für die \n",
    "            Gewichte (`sample_weight_updates`) und Biases (`sample_bias_updates`) berechnet.\n",
    "            Die Summen aller Gewichtsupdates (`weight_updates`) und Bias Updates (`bias_updates`)\n",
    "            werden anschliessend gemittelt und mit der Learning Rate multipliziert, bevor sie auf\n",
    "            die Parameter angewandt werden.\n",
    "        \"\"\"\n",
    "        # Der Inputlayer wird ignoriert, da er die Eingaben aus dem Trainingsset nicht verändert\n",
    "        weight_gradients = [np.zeros(layer.weights.shape)\n",
    "                            for layer in self.layers[1:]]\n",
    "        bias_gradients = [np.zeros(layer.biases.shape)\n",
    "                          for layer in self.layers[1:]]\n",
    "\n",
    "        # Summe aller Gewichts- und Bias Updates\n",
    "        for sample in input_samples:\n",
    "            sample_weight_gradients, sample_bias_gradients = self._backpropagate(\n",
    "                sample)\n",
    "            # Addiert zu den Gradienten\n",
    "            weight_gradients = np.add(\n",
    "                weight_gradients, sample_weight_gradients)\n",
    "            bias_gradients = np.add(bias_gradients, sample_bias_gradients)\n",
    "\n",
    "        # Durchschnitt über alle Gewichts- und Bias Updates\n",
    "        # Der Einfluss der Updates wird durch die `learning_rate` beeinflusst\n",
    "        for layer, layer_weight_gradients, layer_bias_gradients in zip(\n",
    "            self.layers[1:], weight_gradients, bias_gradients\n",
    "        ):\n",
    "            layer.weights += (\n",
    "                self.learning_rate *\n",
    "                layer_weight_gradients / len(input_samples)\n",
    "            )\n",
    "            layer.biases += (\n",
    "                self.learning_rate * layer_bias_gradients / len(input_samples)\n",
    "            )\n",
    "\n",
    "    def _backpropagate(self, training_sample):\n",
    "        \"\"\" Berechnet effizient die Gradienten der Netzparameter\n",
    "\n",
    "            Der Schritt wird für ein `training_sample` durchgeführt.\n",
    "            Dem Backpropagation Schritt geht zunächst der Feedforward Schritt voraus.\n",
    "            Anschließend wird der Gradientenvektor der Kostenfunktion in Abhängigkeit der Aktivierungen im\n",
    "            Output Layer mit `compute_cost_gradients` berechnet und durch die Hidden Layer rückwärts propagiert.\n",
    "            Die Rückgabe ist eine Liste mit den Gradienten der Gewichte und Biases für jeden Layer.\n",
    "        \"\"\"\n",
    "        train_input, train_output = training_sample\n",
    "        self._feed_forward(train_input)\n",
    "        # Berechnet die Gradienten des letzen Layer in Abhängigkeit der Kostenfunktion\n",
    "        gradients = self.layers[-1].compute_cost_gradients(\n",
    "            train_output, cost_func=self.cost_func\n",
    "        )\n",
    "\n",
    "        # Nur für die Hidden Layer werden die Gradienten rückwärts durch das Netz propagiert\n",
    "        for layer in reversed(self.layers[1:-1]):\n",
    "            # `gradients` wird mit jedem Layer überschrieben\n",
    "            gradients = layer.feed_backwards(gradients)\n",
    "\n",
    "        # Akkumuliert alle Gradienten, die mit Backpropagation berechnet wurden\n",
    "        weight_gradients = [\n",
    "            layer.weight_gradients for layer in self.layers[1:]]\n",
    "        bias_gradients = [layer.bias_gradients for layer in self.layers[1:]]\n",
    "        return weight_gradients, bias_gradients\n",
    "\n",
    "    def _stochastic_gradient_descent(\n",
    "        self, training_data, epochs=1, batch_size=1, avg_lookbehind=None\n",
    "    ):\n",
    "        \"\"\" Trainiert das Netz durch iteratives Anpassen der Parameter nach dem\n",
    "            Stochastic Gradient Descent Verfahren\n",
    "\n",
    "            Mit jeder Epoche wird das gesamte Trainingsset einmal durchlaufen.\n",
    "            Nach jeder Epoche wird das Trainingsset durchgemischt und die Parameter (Gewichte und Biases)\n",
    "            *für jeden Batch* aktualisiert sowie der `loss` und die `accuracy` berechnet.\n",
    "            Die Fehler und Accuracies während des Trainingsprozesses werden in `losses` und `accuracies`\n",
    "            akkumuliert und zurückgegeben, falls man diese grafisch darstellen möchte.\n",
    "        \"\"\"\n",
    "        losses, accuracies = list(), list()\n",
    "        training_set_size = len(training_data)\n",
    "\n",
    "        # Wir bilden den running average aus loss und accuracy\n",
    "        # Falls nicht anders spezifiziert, wird dieser aus den letzen 10% der batches gebildet\n",
    "        avg_lookbehind = avg_lookbehind or int(\n",
    "            0.10 * training_set_size / batch_size)\n",
    "        running_loss, running_acc = [], []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Mischt `training_data`\n",
    "            # Dadurch sind die Batches mit jedem Epoch anders zusammengestellt\n",
    "            random.shuffle(training_data)\n",
    "\n",
    "            with tqdm(total=training_set_size) as progress:\n",
    "                for t in range(0, training_set_size, batch_size):\n",
    "                    batch = training_data[t: t + batch_size]\n",
    "\n",
    "                    # Aktualisiert die Parameter des Netzes\n",
    "                    self._update_parameters(batch)\n",
    "                    loss, accuracy = (\n",
    "                        self._calculate_loss(batch),\n",
    "                        self._calculate_accuracy(batch),\n",
    "                    )\n",
    "\n",
    "                    # Bildet den running average für loss und accuracy aus den letzten `avg_lookbehind` Werten\n",
    "                    running_loss = ([loss] + running_loss)[:avg_lookbehind]\n",
    "                    running_acc = ([accuracy] + running_acc)[:avg_lookbehind]\n",
    "                    running_loss_avg = np.average(running_loss)\n",
    "                    running_acc_avg = np.average(running_acc)\n",
    "                    losses.append(running_loss_avg)\n",
    "                    accuracies.append(running_acc_avg)\n",
    "\n",
    "                    # Aktualsiert die progressbar\n",
    "                    progress.set_description(f\"Epoch {epoch+1}\")\n",
    "                    progress.set_postfix(\n",
    "                        loss=\"{0:.3f}\".format(running_loss_avg),\n",
    "                        accuracy=\"{0:.2f}\".format(running_acc_avg),\n",
    "                    )\n",
    "\n",
    "                    progress.update(len(batch))\n",
    "        return losses, accuracies\n",
    "\n",
    "    def fit(self, train_images, train_labels, epochs=1, batch_size=1):\n",
    "        \"\"\" Startet den Trainingsprozess\n",
    "\n",
    "            Das Netz wird mit Stochastic Gradient Descent trainiert,\n",
    "            wodurch die Trainingszeit erheblich verringert wird.\n",
    "        \"\"\"\n",
    "        # Preprocessing der Trainingsdaten durch den Input Layer\n",
    "        train_images, train_labels = self.layers[0].prepare_inputs(\n",
    "            train_images, train_labels\n",
    "        )\n",
    "        training_data = list(zip(train_images, train_labels))\n",
    "        losses, accuracies = self._stochastic_gradient_descent(\n",
    "            training_data, epochs=epochs, batch_size=batch_size\n",
    "        )\n",
    "        return losses, accuracies\n",
    "\n",
    "    def predict(self, model_inputs):\n",
    "        \"\"\" Prognostiziert eine Ausgabe für beliebig viele Eingaben\n",
    "\n",
    "            Die \"Vorhersage\" des Netzes entspricht den Aktivierungen im Output Layer\n",
    "            `self.layers[-1]` nach dem Feedforward Schritt.\n",
    "        \"\"\"\n",
    "        # Preprocessing der `model_inputs` durch den Input Layer\n",
    "        model_inputs = self.layers[0].prepare_inputs(model_inputs)\n",
    "        predicted = np.zeros(\n",
    "            (model_inputs.shape[0], self.layers[-1].neuron_count, 1))\n",
    "        for i, model_input in enumerate(model_inputs):\n",
    "            predicted[i] = self._feed_forward(model_input)\n",
    "        return predicted\n",
    "\n",
    "    def evaluate(self, validation_images, validation_labels):\n",
    "        \"\"\" Evaluiert das Netz auf Validierungsdaten\n",
    "\n",
    "            In diesem Schritt werden `loss` und `accuracy` der Validierungsdaten berechnet,\n",
    "            da diese für die Bewertung des Netzes am aussagekräftigsten sind. \n",
    "        \"\"\"\n",
    "        # Preprocessing der Validierungsdaten durch den Input Layer\n",
    "        validation_images, validation_labels = self.layers[0].prepare_inputs(\n",
    "            validation_images, validation_labels\n",
    "        )\n",
    "        validation_data = list(zip(validation_images, validation_labels))\n",
    "        return (\n",
    "            self._calculate_loss(validation_data),\n",
    "            self._calculate_accuracy(validation_data),\n",
    "        )\n",
    "\n",
    "    def compile(self, learning_rate=None, loss=None):\n",
    "        \"\"\" Kompiliert das Netz\n",
    "\n",
    "            Dieses Netz implementiert nur Stochastic Gradient Descent, weshalb anders als\n",
    "            bei `keras` kein Optimizer übergeben wird.\n",
    "            Diese Funktion dient nur der Konfiguration von Parametern,\n",
    "            die während des Trainings nicht mehr verändert werden.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate or self.learning_rate\n",
    "        self.cost_func = loss or self.cost_func\n",
    "\n",
    "    def inspect(self):\n",
    "        \"\"\" Inspiziert das Netz \"\"\"\n",
    "        print(f\"--------- {self.__class__.__name__} ---------\")\n",
    "        print(f\"  # Inputs: {self.layers[0].neuron_count}\")\n",
    "        for layer in self.layers:\n",
    "            layer.inspect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ZMhWvbkQvKS"
   },
   "outputs": [],
   "source": [
    "def plot_loss_and_accuracy(losses, accuracies, xlabel):\n",
    "    \"\"\" Plottet den Verlauf der `loss` und `accuracy` Werte \"\"\"\n",
    "    plt.plot(losses, label=\"loss\")\n",
    "    plt.plot(accuracies, label=\"accuracy\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylim(top=1, bottom=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uu9mgJsCQVFX"
   },
   "source": [
    "## Beispielnetz für das XOR Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "colab_type": "code",
    "id": "s9zVpEfhQ1ZB",
    "outputId": "c08693d5-557e-4235-a9de-6e42400f685e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                    | 0/80 [00:00<?, ?it/s]<ipython-input-8-cba4ab1cfe47>:81: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  weight_gradients = np.add(\n",
      "<ipython-input-8-cba4ab1cfe47>:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  bias_gradients = np.add(bias_gradients, sample_bias_gradients)\n",
      "Epoch 1: 100%|█████████████████████████████████████| 80/80 [00:00<00:00, 1499.02it/s, accuracy=0.25, loss=0.264]\n",
      "Epoch 2: 100%|█████████████████████████████████████| 80/80 [00:00<00:00, 1280.09it/s, accuracy=0.75, loss=0.232]\n",
      "Epoch 3: 100%|█████████████████████████████████████| 80/80 [00:00<00:00, 1498.61it/s, accuracy=0.50, loss=0.255]\n",
      "  0%|                                                                                    | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- ScratchNet ---------\n",
      "  # Inputs: 2\n",
      "--------- Layer L=0 ---------\n",
      "  # Neuronen: 2\n",
      "    Neuron 0\n",
      "    Neuron 1\n",
      "--------- Layer L=1 ---------\n",
      "  # Neuronen: 4\n",
      "    Neuron 0\n",
      "      Weight: -0.5569604417439836\n",
      "      Weight: 0.8436415688968443\n",
      "      Bias: 0.31196902612489275\n",
      "    Neuron 1\n",
      "      Weight: 0.08768656030615152\n",
      "      Weight: 0.4440858980393146\n",
      "      Bias: 0.9298126600363446\n",
      "    Neuron 2\n",
      "      Weight: -0.17985546443522166\n",
      "      Weight: 0.1557512356338521\n",
      "      Bias: -1.0288913682333445\n",
      "    Neuron 3\n",
      "      Weight: -1.1412771652020859\n",
      "      Weight: -1.4196403563339317\n",
      "      Bias: 0.5599854834756356\n",
      "--------- Layer L=2 ---------\n",
      "  # Neuronen: 2\n",
      "    Neuron 0\n",
      "      Weight: -0.9039249889233043\n",
      "      Weight: -0.2093218147844362\n",
      "      Weight: 0.6558365376294598\n",
      "      Weight: -0.8551824053543718\n",
      "      Bias: 1.6574285532766326\n",
      "    Neuron 1\n",
      "      Weight: -0.15830487981734487\n",
      "      Weight: 0.9227782286029631\n",
      "      Weight: 2.052239245289835\n",
      "      Weight: 0.05998909471777869\n",
      "      Bias: 0.041863375569344864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|█████████████████████████████████████| 80/80 [00:00<00:00, 1159.41it/s, accuracy=0.50, loss=0.249]\n",
      "Epoch 5: 100%|█████████████████████████████████████| 80/80 [00:00<00:00, 1706.61it/s, accuracy=0.88, loss=0.235]\n",
      "Epoch 6: 100%|█████████████████████████████████████| 80/80 [00:00<00:00, 2118.96it/s, accuracy=0.75, loss=0.237]\n",
      "Epoch 7: 100%|█████████████████████████████████████| 80/80 [00:00<00:00, 1706.79it/s, accuracy=0.50, loss=0.252]\n",
      "Epoch 8: 100%|█████████████████████████████████████| 80/80 [00:00<00:00, 1498.65it/s, accuracy=0.00, loss=0.255]\n",
      "Epoch 9: 100%|█████████████████████████████████████| 80/80 [00:00<00:00, 1280.08it/s, accuracy=0.62, loss=0.241]\n",
      "Epoch 10: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1498.60it/s, accuracy=0.38, loss=0.251]\n",
      "Epoch 11: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1280.09it/s, accuracy=0.75, loss=0.240]\n",
      "Epoch 12: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1706.61it/s, accuracy=0.62, loss=0.224]\n",
      "Epoch 13: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1498.79it/s, accuracy=0.38, loss=0.246]\n",
      "Epoch 14: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1279.98it/s, accuracy=0.38, loss=0.256]\n",
      "Epoch 15: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1159.33it/s, accuracy=0.62, loss=0.251]\n",
      "Epoch 16: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1706.78it/s, accuracy=0.62, loss=0.241]\n",
      "Epoch 17: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1159.30it/s, accuracy=0.88, loss=0.222]\n",
      "Epoch 18: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1498.77it/s, accuracy=0.50, loss=0.245]\n",
      "Epoch 19: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1280.00it/s, accuracy=0.62, loss=0.244]\n",
      "Epoch 20: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1498.58it/s, accuracy=0.75, loss=0.230]\n",
      "Epoch 21: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1024.08it/s, accuracy=0.62, loss=0.232]\n",
      "Epoch 22: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1498.61it/s, accuracy=0.75, loss=0.208]\n",
      "Epoch 23: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1498.75it/s, accuracy=0.75, loss=0.245]\n",
      "Epoch 24: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1706.77it/s, accuracy=0.88, loss=0.214]\n",
      "Epoch 25: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1498.66it/s, accuracy=0.50, loss=0.256]\n",
      "Epoch 26: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1706.79it/s, accuracy=0.88, loss=0.224]\n",
      "Epoch 27: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1159.31it/s, accuracy=0.50, loss=0.243]\n",
      "Epoch 28: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1498.78it/s, accuracy=0.62, loss=0.227]\n",
      "Epoch 29: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1581.73it/s, accuracy=0.88, loss=0.178]\n",
      "Epoch 30: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1230.67it/s, accuracy=0.62, loss=0.238]\n",
      "Epoch 31: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1280.09it/s, accuracy=0.75, loss=0.248]\n",
      "Epoch 32: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1498.61it/s, accuracy=1.00, loss=0.224]\n",
      "Epoch 33: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1280.09it/s, accuracy=0.75, loss=0.233]\n",
      "Epoch 34: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 2118.74it/s, accuracy=0.50, loss=0.236]\n",
      "Epoch 35: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1280.10it/s, accuracy=0.62, loss=0.232]\n",
      "Epoch 36: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1498.63it/s, accuracy=0.88, loss=0.226]\n",
      "Epoch 37: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1498.77it/s, accuracy=0.62, loss=0.230]\n",
      "Epoch 38: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1706.61it/s, accuracy=0.88, loss=0.217]\n",
      "Epoch 39: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1498.76it/s, accuracy=0.88, loss=0.208]\n",
      "Epoch 40: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1706.61it/s, accuracy=0.62, loss=0.219]\n",
      "Epoch 41: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1498.74it/s, accuracy=0.75, loss=0.223]\n",
      "Epoch 42: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1706.60it/s, accuracy=0.88, loss=0.172]\n",
      "Epoch 43: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1498.76it/s, accuracy=0.50, loss=0.222]\n",
      "Epoch 44: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1706.56it/s, accuracy=0.75, loss=0.181]\n",
      "Epoch 45: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1706.79it/s, accuracy=0.75, loss=0.213]\n",
      "Epoch 46: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 2118.72it/s, accuracy=0.75, loss=0.200]\n",
      "Epoch 47: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1706.80it/s, accuracy=0.88, loss=0.172]\n",
      "Epoch 48: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1498.61it/s, accuracy=1.00, loss=0.196]\n",
      "Epoch 49: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1706.80it/s, accuracy=0.88, loss=0.169]\n",
      "Epoch 50: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 2118.70it/s, accuracy=1.00, loss=0.167]\n",
      "Epoch 51: 100%|████████████████████████████████████| 80/80 [00:00<00:00, 1706.79it/s, accuracy=1.00, loss=0.143]\n",
      "Epoch 52:  65%|███████████████████████▍            | 52/80 [00:00<00:00, 2349.75it/s, accuracy=1.00, loss=0.146]"
     ]
    }
   ],
   "source": [
    "xor_train_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "xor_train_labels = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "xor_total_classes = 2\n",
    "xor_train_vec_labels = tf.keras.utils.to_categorical(xor_train_labels, xor_total_classes)\n",
    "\n",
    "xorModel = ScratchNet(\n",
    "    [\n",
    "        FlattenLayer(input_shape=(2, 1)),\n",
    "        DenseLayer(4, activation=Sigmoid()),\n",
    "        DenseLayer(2, activation=Sigmoid()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Zeigt die Struktur und Initialisierung des Netzes\n",
    "xorModel.inspect()\n",
    "\n",
    "# Wiederholt die Werte um weniger Epochen trainieren zu müssen\n",
    "repeat = (20, 1)\n",
    "xor_train_inputs = np.tile(xor_train_inputs, repeat)\n",
    "xor_train_vec_labels = np.tile(xor_train_vec_labels, repeat)\n",
    "\n",
    "xorModel.compile(learning_rate=0.8, loss=SquaredError())\n",
    "\n",
    "start = time.time()\n",
    "xor_losses, xor_accuracies = xorModel.fit(\n",
    "    xor_train_inputs, xor_train_vec_labels, epochs=100, batch_size=4\n",
    ")\n",
    "end = time.time()\n",
    "print(\"Trainingsdauer: {:.1f}s\".format(end - start))\n",
    "\n",
    "xor_val_loss, xor_val_acc = xorModel.evaluate(\n",
    "    validation_images=xor_train_inputs, validation_labels=xor_train_vec_labels\n",
    ")\n",
    "print(f\"Validation loss: {xor_val_loss}\")\n",
    "print(f\"Validation accuracy: {xor_val_acc}\")\n",
    "\n",
    "plot_loss_and_accuracy(xor_losses, xor_accuracies, xlabel=\"epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "scratchNet-full-commented-solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
