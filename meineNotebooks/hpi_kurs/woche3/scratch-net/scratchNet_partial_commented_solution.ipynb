{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scratchNet-partial-commented-solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52hHPx_BQQZ7",
        "colab_type": "text"
      },
      "source": [
        "# ScratchNet\n",
        "Ein einfaches künstliches neuronales Netz mit einer von `keras` inspirierten API.\n",
        "Hinweis: Das Netz wurde ausschließlich für Lernzwecke verfasst."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzSm4SyFRe4Q",
        "colab_type": "code",
        "outputId": "21e76afd-fc12-4523-feba-c898d18f322d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "!pip install --upgrade deeplearning2020"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: deeplearning2020 in /usr/local/lib/python3.6/dist-packages (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: kerasltisubmission>=0.4.5 in /usr/local/lib/python3.6/dist-packages (from deeplearning2020) (0.4.5)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from kerasltisubmission>=0.4.5->deeplearning2020) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: progressbar2 in /usr/local/lib/python3.6/dist-packages (from kerasltisubmission>=0.4.5->deeplearning2020) (3.38.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from kerasltisubmission>=0.4.5->deeplearning2020) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->kerasltisubmission>=0.4.5->deeplearning2020) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kerasltisubmission>=0.4.5->deeplearning2020) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kerasltisubmission>=0.4.5->deeplearning2020) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->kerasltisubmission>=0.4.5->deeplearning2020) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->kerasltisubmission>=0.4.5->deeplearning2020) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from progressbar2->kerasltisubmission>=0.4.5->deeplearning2020) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOixRSglQDcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import abc\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "from tqdm import tqdm, trange\n",
        "from deeplearning2020 import helpers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6c8fBcRQX3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DifferentiableFunction(abc.ABC):\n",
        "    \"\"\" Abstrakte Klasse einer differenzierbaren Funktion\n",
        "        Die Implementierung der Methoden erfolgt durch Spezialisierung.\n",
        "    \"\"\"\n",
        "\n",
        "    def derivative(self, net_input):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, net_input):\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYf4TAzdQall",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sigmoid(DifferentiableFunction):\n",
        "    \"\"\" Sigmoid Aktivierungsfunktion\n",
        "        Stetige und differenzierbare Funktion, dessen Graph der Treppenfunktion ähnelt.\n",
        "    \"\"\"\n",
        "\n",
        "    def derivative(self, net_input):\n",
        "        return self(net_input) * (1 - self(net_input))\n",
        "\n",
        "    def __call__(self, net_input):\n",
        "        return 1 / (1 + np.exp(-net_input))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX6Hw7hAQduw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SquaredError(DifferentiableFunction):\n",
        "    \"\"\" Quadratische Fehlerfunktion\n",
        "        Durch das Quadrieren wird sichergestellt, dass der Fehler nicht negativ wird\n",
        "        und höhere Differenzen stärker ins Gewicht fallen.\n",
        "    \"\"\"\n",
        "\n",
        "    def derivative(self, target, actual):\n",
        "        # TODO: Implement me!\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def __call__(self, target, actual):\n",
        "        # TODO: Implement me!\n",
        "        raise NotImplementedError()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gczgOtCWQh0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DenseLayer:\n",
        "    \"\"\" DenseLayer\n",
        "\n",
        "        Implementiert einen Layer, dessen Neuronen mit jeweils allen Neuronen aus der Schicht davor verbunden sind. \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        neuron_count,\n",
        "        depth=None,\n",
        "        activation=None,\n",
        "        biases=None,\n",
        "        weights=None,\n",
        "        prev_layer=None,\n",
        "        next_layer=None,\n",
        "    ):\n",
        "        \"\"\" Initialisiert DenseLayer L \"\"\"\n",
        "        self.depth = depth\n",
        "        self.next_layer = next_layer\n",
        "        self.prev_layer = prev_layer\n",
        "\n",
        "        self.neuron_count = neuron_count\n",
        "        self.activation_func = activation or Sigmoid()\n",
        "\n",
        "        self.weights = weights\n",
        "        self.biases = biases\n",
        "\n",
        "    def prepare_inputs(self, images, labels=None):\n",
        "        \"\"\" Bereitet die Eingabedaten für den DenseLayer L vor\n",
        "\n",
        "            Ein `DenseLayer` benötigt keine besondere Vorbereitung der Eingabe, anders als zum Beispiel der `FlattenLayer`.\n",
        "        \"\"\"\n",
        "        return images if labels is None else images, labels\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        \"\"\" Initialisiert die Netzparameter für DenseLayer L\n",
        "\n",
        "            Die Parameter der Layer werden zufällig gewählt.\n",
        "            Die Gewichte von L werden durch eine Matrix mit `shape=(n_L, n_P)` repräsentiert,\n",
        "            wobei `n_L` die Anzahl an Neuronen des aktuellen Layer `L` und `n_P` die Anzahl an Neuronen im vorigen Layer `P = L - 1` ist.\n",
        "            Der Eintrag `(i,j)` enspricht dem Gewicht, mit dem die Neuronen `i` (Layer `L`) und `j` (Layer `P = L - 1`) miteinander verbunden sind.\n",
        "\n",
        "            Die Biases von L werden durch eine Matrix mit `shape=(n_L, 1)` repräsentiert,\n",
        "            wobei `n_L` die Anzahl an Neuronen des aktuellen Layer `L` ist.\n",
        "            Der Eintrag `(i,1)` enspricht dem Bias, der zu dem Neuron `i` im aktuellen Layer `L` addiert / subtrahiert wird.\n",
        "            Da die Subtraktion des Biases `b_i = biases[i,1]` der Addition des negierten Bias `-b_i` entspricht, spielt dies für das Netz keine Rolle.\n",
        "        \"\"\"\n",
        "        # `np.random.randn(d0, d1, ..., dn)` initialisiert eine Matrix mit shape (d0, d1, ..., dn) mit Werten aus der Standard Normalverteilung\n",
        "        if self.weights is None:\n",
        "            self.weights = np.random.randn(\n",
        "                self.neuron_count, self.prev_layer.neuron_count\n",
        "            )\n",
        "        if self.biases is None:\n",
        "            self.biases = np.random.randn(self.neuron_count, 1)\n",
        "\n",
        "    def compute_cost_gradients(self, label_vec, cost_func):\n",
        "        \"\"\" Bestimmt die Gradienten ∇aLC der Kostenfunktion C in Abhängigkeit der \n",
        "            Aktivierungen in DenseLayer L\n",
        "\n",
        "            Dieser lässt sich sehr einfach ohne Abhängigkeiten berechnen:\n",
        "            Dazu wird der Gradientenvektor der Kostenfunktion für ein Trainingssample mit dem\n",
        "            Gradientenvektor der Aktivierungen `self.activation_vec` in Layer `L` multipliziert,\n",
        "            da die Aktivierungen als Eingabe der Kostenfunktion diese direkt beeinflussen.\n",
        "            Diese Gradienten bilden dann die Startwerte für den Backpropagation Schritt;\n",
        "            `compute_cost_gradients` wird demnach nur auf dem Output Layer aufgerufen.\n",
        "            Anschließend werden die Gradienten der Weights und Biases des Layers in \n",
        "            `_update_layer_gradients` aktualisiert.\n",
        "        \"\"\"\n",
        "        # TODO: Implement me!\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def feed_backwards(self, prev_input_gradients):\n",
        "        \"\"\" Feed-backward Schritt für DenseLayer L zur Berechnung des neuen Eingabegradienten δL\n",
        "\n",
        "            Im Backpropagation Schritt wird der Fehler des Netzes rückwärts durch das Netz\n",
        "            propagiert. Da die Werte rückwärts propagiert werden, ist der vorherige Layer \n",
        "            `self.next_layer` (L+1). Es wird das Produkt aus der transponierten Gewichtsmatrix \n",
        "            (WL)T des vorherigen Layer mit den Gradienten δL+1 aus dem letzen `feed_backwards` Schritt\n",
        "            gebildet und mit dem Gradientenvektor der Aktivierungen in Layer L multipliziert.\n",
        "            Die Gewichtsmatrix hat eine `shape=(n_L+1, n_L)`, und wird transponiert, da wir nun rückwärts\n",
        "            durch das Netz propagieren und mit den Gradienten aus dem vorigen Layer L+1\n",
        "            mit `shape=(n_L+1, 1)` multiplizieren wollen. Durch das Transponieren erhalten wir \n",
        "            eine `shape=(n_L, n_L+1)` so, dass das Ergebnis mit dem Gradientenvektor der Aktivierungen\n",
        "            in Layer L mit `shape=(n_L+1, 1)` multiplizieren können.\n",
        "\n",
        "            Als Errinnerung: `self.layer_inputs` ist der Vektor, dessen Einträge jeweils den \n",
        "            summierten Eingaben der Neuronen in Layer L entsprechen.\n",
        "            Anschließend werden die Gradienten der Weights und Biases des Layers in `_update_layer_gradients`\n",
        "            aktualsiert.\n",
        "        \"\"\"\n",
        "        # TODO: Implement me!\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def feed_forward_layer(self, input_activations):\n",
        "        \"\"\" Feedforward Schritt für DenseLayer L\n",
        "\n",
        "            Als Eingabe werden die Aktivierungen aus dem vorherigen Layer übergeben.\n",
        "            Zunächst werden die summierten Eingaben der jeweiligen Neuronen in Layer L berechnet.\n",
        "            Da es sich um einen `DenseLayer` handelt wird die Gewichtsmatrix der Kanten zwischen den \n",
        "            Layern P und L (`shape=(n_L, n_P)`) mit den `input_activations` (`shape=(n_P, 1)` mit P=L-1)\n",
        "            aller Neuronen aus dem vorherigen Layer multipliziert.\n",
        "            Auf die summierten Eingaben der Neuronen (`shape=(n_L, 1)`) werden dann die \n",
        "            Biases des Layer L (`shape=(n_L, 1)`) addiert und as Ergebnis in `self.layer_inputs` zwischengespeichert.\n",
        "            Abschließend werden auch die Aktivierungen der Neuronen in Layer L berechnet, indem die Aktivierungsfunktion\n",
        "            auf die summierten Eingaben der Neuronen angewandt wird.\n",
        "        \"\"\"\n",
        "        self.layer_inputs = np.dot(\n",
        "            self.weights, input_activations) + self.biases\n",
        "        self.activation_vec = self.activation_func(self.layer_inputs)\n",
        "        return self.activation_vec\n",
        "\n",
        "    def inspect(self):\n",
        "        \"\"\" Inspiziert DenseLayer L \"\"\"\n",
        "        print(f\"--------- Layer L={self.depth} ---------\")\n",
        "        print(f\"  # Neuronen: {self.neuron_count}\")\n",
        "        for n in range(self.neuron_count):\n",
        "            print(f\"    Neuron {n}\")\n",
        "            if self.prev_layer:\n",
        "                for w in self.weights[n]:\n",
        "                    print(f\"      Weight: {w}\")\n",
        "                print(f\"      Bias: {self.biases[n][0]}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKqSvjCgQjHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FlattenLayer(DenseLayer):\n",
        "    def __init__(self, input_shape):\n",
        "        \"\"\" Initialisiert den FlattenLayer\n",
        "\n",
        "            Die Unterscheidung zu einem `DenseLayer` ist in unserer Implementierung fast nur semantisch.\n",
        "            Lediglich die Anzahl der Neuronen wird durch die `input_shape` vorgegeben und die Eingaben des\n",
        "            Layers werden vorher mit `reshape` auf den Input Layer angepasst.  \n",
        "        \"\"\"\n",
        "        total_input_neurons = 1\n",
        "        # Beispiele:\n",
        "        # (28,28)   wird zu 28*28=784\n",
        "        # (28,28,1) wird zu 28*28*1=784\n",
        "        # (4,4,2)   wird zu 4*4*2=32\n",
        "        for dim in input_shape:\n",
        "            total_input_neurons *= dim\n",
        "        super().__init__(neuron_count=total_input_neurons)\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        \"\"\" Initialisiert die Netzparameter für FlattenLayer L\n",
        "\n",
        "            Für einen `FlattenLayer` ist keine Initialisierung erforderlich, da dieser nur als\n",
        "            Input Layer verwendet wird und die Aktivierungen des `FlattenLayer` durch die Werte\n",
        "            des aktuellen Trainingsbeispiels (siehe `feed_forward_layer`) gegeben sind.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def feed_forward_layer(self, input_activations):\n",
        "        \"\"\" Führt den Feedforward Schritt für FlattenLayer L durch\n",
        "\n",
        "            Für einen `FlattenLayer`, der als Input Layer dient, sind `input_activations` die Werte\n",
        "            des aktuellen Trainingsbeispiels, welche unverändert als Aktivierungen der Neuronen übernommen werden.\n",
        "        \"\"\"\n",
        "        self.activation_vec = input_activations\n",
        "        return input_activations\n",
        "\n",
        "    def prepare_inputs(self, images, labels=None):\n",
        "        \"\"\" Bereitet die Eingabedaten für FlattenLayer L vor\n",
        "\n",
        "            Der `FlattenLayer` nutzt diese Methode um die Eingabematrizen mit `reshape` in eine flache Form zu bringen.\n",
        "            Die Shape der Einträge dieser flachen Matrix ist durch `self.neuron_count` vorgegeben.\n",
        "            Zudem werden die Eingabematrizen wie Labels (falls vorhanden) um eine Dimension erweitert,\n",
        "            um auf jeden Fall transponiert werden zu können.\n",
        "        \"\"\"\n",
        "        flattened_images = images.reshape(\n",
        "            images.shape[0], self.neuron_count, 1)\n",
        "        if labels is not None:\n",
        "            labels = labels.reshape(labels.shape[0], -1, 1)\n",
        "            return flattened_images, labels\n",
        "        return flattened_images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vB7U79cQsSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ScratchNet:\n",
        "    def __init__(self, layers):\n",
        "        \"\"\" Initialisert das Netz\n",
        "\n",
        "            Die Layer des Netzes werden miteinander verkettet,\n",
        "            sodass jeder Layer L seinen Index und eine Referenz auf seine Nachbarn erhält.\n",
        "            Anschließend können die Parameter der Layer initialisiert werden.\n",
        "        \"\"\"\n",
        "        self.learning_rate = 0.5\n",
        "        self.cost_func = SquaredError()\n",
        "        self.layers = layers\n",
        "        for index, layer in enumerate(self.layers):\n",
        "            layer.prev_layer = self.layers[index - 1] if index > 0 else None\n",
        "            layer.next_layer = (\n",
        "                self.layers[index + 1] if index +\n",
        "                1 < len(self.layers) else None\n",
        "            )\n",
        "            layer.depth = index\n",
        "            layer.initialize_parameters()\n",
        "\n",
        "    def _calculate_loss(self, input_samples):\n",
        "        \"\"\" Berechnet den Fehler des Netzes auf den übergebenen `input_samples`\n",
        "\n",
        "            Der Fehler ist dabei definiert als der Durchschnitt über den Fehlern der \n",
        "            `input_samples`. Um den Fehler eines Samples zu berechnen, wird dessen Ausgabe\n",
        "            im Feedforward Schritt berechnet und mit dem erwarteten Label als Argument der\n",
        "            Kostenfunktion `cost_func` übergeben.\n",
        "        \"\"\"\n",
        "        # TODO: Implement me!\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def _calculate_accuracy(self, input_samples):\n",
        "        \"\"\" Berechnet die Accuracy des Netzes auf den übergebenen `input_samples`\n",
        "\n",
        "            Für jedes Sample wird im Feedforward Schritt die Ausgabe des Netzes berechnet und\n",
        "            mithilfe der `np.argmax` Funktion, welche für eine Matrix den Index des Eintrags\n",
        "            mit dem höchsten Wert berechnet, mit dem erwarteten Label verglichen.\n",
        "            Dabei haben sowohl die Aktivierungsmatrix im Output Layer als auch die Matrix des \n",
        "            erwarteten Label eine `shape=(nL,1)` wobei `nL` die Zahl der Neuronen im Output Layer L sind.\n",
        "            Die Accuracy ist dann definiert als der Quotient `num_correct/len(input_samples)`.\n",
        "        \"\"\"\n",
        "        # TODO: Implement me!\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def _feed_forward(self, input_sample):\n",
        "        \"\"\" Feedforward eines Samples durch das Netz \"\"\"\n",
        "        for layer in self.layers:\n",
        "            # `input_sample` wird mit jedem Layer überschrieben\n",
        "            # Jeder Layer speichert dabei die Aktivierungen seiner Neuronen in `self.activation_vec`\n",
        "            input_sample = layer.feed_forward_layer(input_sample)\n",
        "        return input_sample\n",
        "\n",
        "    def _update_parameters(self, input_samples):\n",
        "        \"\"\" Aktualisiert die Parameter (Weights und Biases) des Netzes\n",
        "\n",
        "            Für jedes Input Sample werden mit Backpropagation die Gradienten für die \n",
        "            Gewichte (`sample_weight_updates`) und Biases (`sample_bias_updates`) berechnet.\n",
        "            Die Summen aller Gewichtsupdates (`weight_updates`) und Bias Updates (`bias_updates`)\n",
        "            werden anschliessend gemittelt und mit der Learning Rate multipliziert, bevor sie auf\n",
        "            die Parameter angewandt werden.\n",
        "        \"\"\"\n",
        "        # Der Inputlayer wird ignoriert, da er die Eingaben aus dem Trainingsset nicht verändert\n",
        "        weight_gradients = [np.zeros(layer.weights.shape)\n",
        "                            for layer in self.layers[1:]]\n",
        "        bias_gradients = [np.zeros(layer.biases.shape)\n",
        "                          for layer in self.layers[1:]]\n",
        "\n",
        "        # Summe aller Gewichts- und Bias Updates\n",
        "        for sample in input_samples:\n",
        "            sample_weight_gradients, sample_bias_gradients = self._backpropagate(\n",
        "                sample)\n",
        "            # Addiert zu den Gradienten\n",
        "            weight_gradients = np.add(\n",
        "                weight_gradients, sample_weight_gradients)\n",
        "            bias_gradients = np.add(bias_gradients, sample_bias_gradients)\n",
        "\n",
        "        # Durchschnitt über alle Gewichts- und Bias Updates\n",
        "        # Der Einfluss der Updates wird durch die `learning_rate` beeinflusst\n",
        "        for layer, layer_weight_gradients, layer_bias_gradients in zip(\n",
        "            self.layers[1:], weight_gradients, bias_gradients\n",
        "        ):\n",
        "            layer.weights += (\n",
        "                self.learning_rate *\n",
        "                layer_weight_gradients / len(input_samples)\n",
        "            )\n",
        "            layer.biases += (\n",
        "                self.learning_rate * layer_bias_gradients / len(input_samples)\n",
        "            )\n",
        "\n",
        "    def _backpropagate(self, training_sample):\n",
        "        \"\"\" Berechnet effizient die Gradienten der Netzparameter\n",
        "\n",
        "            Der Schritt wird für ein `training_sample` durchgeführt.\n",
        "            Dem Backpropagation Schritt geht zunächst der Feedforward Schritt voraus.\n",
        "            Anschließend wird der Gradientenvektor der Kostenfunktion in Abhängigkeit der Aktivierungen im\n",
        "            Output Layer mit `compute_cost_gradients` berechnet und durch die Hidden Layer rückwärts propagiert.\n",
        "            Die Rückgabe ist eine Liste mit den Gradienten der Gewichte und Biases für jeden Layer.\n",
        "        \"\"\"\n",
        "        train_input, train_output = training_sample\n",
        "        self._feed_forward(train_input)\n",
        "        # Berechnet die Gradienten des letzen Layer in Abhängigkeit der Kostenfunktion\n",
        "        gradients = self.layers[-1].compute_cost_gradients(\n",
        "            train_output, cost_func=self.cost_func\n",
        "        )\n",
        "\n",
        "        # Nur für die Hidden Layer werden die Gradienten rückwärts durch das Netz propagiert\n",
        "        for layer in reversed(self.layers[1:-1]):\n",
        "            # `gradients` wird mit jedem Layer überschrieben\n",
        "            gradients = layer.feed_backwards(gradients)\n",
        "\n",
        "        # Akkumuliert alle Gradienten, die mit Backpropagation berechnet wurden\n",
        "        weight_gradients = [\n",
        "            layer.weight_gradients for layer in self.layers[1:]]\n",
        "        bias_gradients = [layer.bias_gradients for layer in self.layers[1:]]\n",
        "        return weight_gradients, bias_gradients\n",
        "\n",
        "    def _gradient_descent(self, training_data, epochs=1):\n",
        "        \"\"\" Trainiert das Netz durch iteratives Anpassen der Netzparameter nach dem\n",
        "            Gradient Descent Verfahren\n",
        "\n",
        "            Mit jeder Epoche wird das gesamte Trainingsset einmal durchlaufen.\n",
        "            Nach jeder Epoche werden die Parameter (Gewichte und Biases) einmal aktualisiert\n",
        "            und der `loss` und die `accuracy` berechnet.\n",
        "            Die Fehler und Accuracies während des Trainingsprozesses werden in `losses` und `accuracies`\n",
        "            akkumuliert und zurückgegeben, falls man diese grafisch darstellen möchte.\n",
        "        \"\"\"\n",
        "        losses, accuracies = list(), list()\n",
        "        for epoch in range(epochs):\n",
        "            self._update_parameters(training_data)\n",
        "            loss, accuracy = (\n",
        "                self._calculate_loss(training_data),\n",
        "                self._calculate_accuracy(training_data),\n",
        "            )\n",
        "\n",
        "            losses.append(loss)\n",
        "            accuracies.append(accuracy)\n",
        "            print(\n",
        "                \"Epoch {0}: loss={1:.3f} acc={2:.2f}\".format(\n",
        "                    epoch + 1, loss, accuracy)\n",
        "            )\n",
        "        return losses, accuracies\n",
        "\n",
        "    def fit(self, train_images, train_labels, epochs=1):\n",
        "        \"\"\" Startet den Trainingsprozess\n",
        "\n",
        "            Das Netz wird mit Gradient Descent trainiert,\n",
        "            wodurch die Trainingszeit erheblich verringert wird.\n",
        "        \"\"\"\n",
        "        # Preprocessing der Trainingsdaten durch den Input Layer\n",
        "        train_images, train_labels = self.layers[0].prepare_inputs(\n",
        "            train_images, train_labels\n",
        "        )\n",
        "        training_data = list(zip(train_images, train_labels))\n",
        "        losses, accuracies = self._gradient_descent(training_data, epochs=epochs)\n",
        "        return losses, accuracies\n",
        "\n",
        "    def predict(self, model_inputs):\n",
        "        \"\"\" Prognostiziert eine Ausgabe für beliebig viele Eingaben\n",
        "\n",
        "            Die \"Vorhersage\" des Netzes entspricht den Aktivierungen im Output Layer\n",
        "            `self.layers[-1]` nach dem Feedforward Schritt.\n",
        "        \"\"\"\n",
        "        # Preprocessing der `model_inputs` durch den Input Layer\n",
        "        model_inputs = self.layers[0].prepare_inputs(model_inputs)\n",
        "        predicted = np.zeros(\n",
        "            (model_inputs.shape[0], self.layers[-1].neuron_count, 1))\n",
        "        for i, model_input in enumerate(model_inputs):\n",
        "            predicted[i] = self._feed_forward(model_input)\n",
        "        return predicted\n",
        "\n",
        "    def evaluate(self, validation_images, validation_labels):\n",
        "        \"\"\" Evaluiert das Netz auf Validierungsdaten\n",
        "\n",
        "            In diesem Schritt werden `loss` und `accuracy` der Validierungsdaten berechnet,\n",
        "            da diese für die Bewertung des Netzes am aussagekräftigsten sind. \n",
        "        \"\"\"\n",
        "        # TODO: Implement me!\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def compile(self, learning_rate=None, loss=None):\n",
        "        \"\"\" Kompiliert das Netz\n",
        "\n",
        "            Dieses Netz implementiert nur Stochastic Gradient Descent, weshalb anders als\n",
        "            bei `keras` kein Optimizer übergeben wird.\n",
        "            Diese Funktion dient nur der Konfiguration von Parametern,\n",
        "            die während des Trainings nicht mehr verändert werden.\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate or self.learning_rate\n",
        "        self.cost_func = loss or self.cost_func\n",
        "\n",
        "    def inspect(self):\n",
        "        \"\"\" Inspiziert das Netz \"\"\"\n",
        "        print(f\"--------- {self.__class__.__name__} ---------\")\n",
        "        print(f\"  # Inputs: {self.layers[0].neuron_count}\")\n",
        "        for layer in self.layers:\n",
        "            layer.inspect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZMhWvbkQvKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_loss_and_accuracy(losses, accuracies, xlabel):\n",
        "    \"\"\" Plottet den Verlauf der `loss` und `accuracy` Werte \"\"\"\n",
        "    plt.plot(losses, label=\"loss\")\n",
        "    plt.plot(accuracies, label=\"accuracy\")\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylim(top=1, bottom=0)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu9mgJsCQVFX",
        "colab_type": "text"
      },
      "source": [
        "## Beispielnetz für das XOR Problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9zVpEfhQ1ZB",
        "colab_type": "code",
        "outputId": "d0d3ccff-8442-4b12-90ae-fbf5047f3108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "source": [
        "xor_train_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "xor_train_labels = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "xor_total_classes = 2\n",
        "xor_train_vec_labels = tf.keras.utils.to_categorical(xor_train_labels, xor_total_classes)\n",
        "\n",
        "xorModel = ScratchNet(\n",
        "    [\n",
        "        FlattenLayer(input_shape=(2, 1)),\n",
        "        DenseLayer(4, activation=Sigmoid()),\n",
        "        DenseLayer(2, activation=Sigmoid()),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Zeigt die Struktur und Initialisierung des Netzes\n",
        "# model.inspect()\n",
        "\n",
        "# Wiederholt die Werte um weniger Epochen trainieren zu müssen\n",
        "repeat = (10000, 1)\n",
        "xor_train_inputs = np.tile(xor_train_inputs, repeat)\n",
        "xor_train_vec_labels = np.tile(xor_train_vec_labels, repeat)\n",
        "\n",
        "xorModel.compile(learning_rate=0.1, loss=SquaredError())\n",
        "\n",
        "start = time.time()\n",
        "xor_losses, xor_accuracies = xorModel.fit(\n",
        "    xor_train_inputs, xor_train_vec_labels, epochs=1\n",
        ")\n",
        "end = time.time()\n",
        "print(\"Trainingsdauer: {:.1f}s\".format(end - start))\n",
        "\n",
        "xor_val_loss, xor_val_acc = xorModel.evaluate(\n",
        "    validation_images=xor_train_inputs, validation_labels=xor_train_vec_labels\n",
        ")\n",
        "print(f\"Validation loss: {xor_val_loss}\")\n",
        "print(f\"Validation accuracy: {xor_val_acc}\")\n",
        "\n",
        "plot_loss_and_accuracy(xor_losses, xor_accuracies, xlabel=\"epochs\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-2e937ee7521c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m xor_losses, xor_accuracies = xorModel.fit(\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mxor_train_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxor_train_vec_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-9173b491b79a>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_images, train_labels, epochs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         )\n\u001b[1;32m    153\u001b[0m         \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-9173b491b79a>\u001b[0m in \u001b[0;36m_gradient_descent\u001b[0;34m(self, training_data, epochs)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             loss, accuracy = (\n\u001b[1;32m    131\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-9173b491b79a>\u001b[0m in \u001b[0;36m_update_parameters\u001b[0;34m(self, input_samples)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             sample_weight_gradients, sample_bias_gradients = self._backpropagate(\n\u001b[0;32m---> 71\u001b[0;31m                 sample)\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;31m# Addiert zu den Gradienten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             weight_gradients = np.add(\n",
            "\u001b[0;32m<ipython-input-21-9173b491b79a>\u001b[0m in \u001b[0;36m_backpropagate\u001b[0;34m(self, training_sample)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# Berechnet die Gradienten des letzen Layer in Abhängigkeit der Kostenfunktion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         gradients = self.layers[-1].compute_cost_gradients(\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mtrain_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         )\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-9514583ac8a4>\u001b[0m in \u001b[0;36mcompute_cost_gradients\u001b[0;34m(self, label_vec, cost_func)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \"\"\"\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# TODO: Implement me!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_backwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_input_gradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFDhYU0gS0il",
        "colab_type": "text"
      },
      "source": [
        "## Beispielnetz für den MNIST Datensatz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFHUBkxsS2bI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(\n",
        "    (mnist_train_images, mnist_train_labels),\n",
        "    (mnist_test_images, mnist_test_labels),\n",
        ") = tf.keras.datasets.mnist.load_data()\n",
        "mnist_train_images = mnist_train_images / 255.0\n",
        "mnist_test_images = mnist_test_images / 255.0\n",
        "\n",
        "mnist_total_classes = 10\n",
        "mnist_train_vec_labels = tf.keras.utils.to_categorical(\n",
        "    mnist_train_labels, mnist_total_classes)\n",
        "mnist_test_vec_labels = tf.keras.utils.to_categorical(mnist_test_labels, mnist_total_classes)\n",
        "\n",
        "mnistModel = ScratchNet(\n",
        "    [\n",
        "        FlattenLayer(input_shape=(28, 28)),\n",
        "        DenseLayer(128, activation=Sigmoid()),\n",
        "        DenseLayer(10, activation=Sigmoid()),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Zeigt die Struktur und Initialisierung des Netzes\n",
        "# model.inspect()\n",
        "\n",
        "mnistModel.compile(learning_rate=5.0, loss=SquaredError())\n",
        "start = time.time()\n",
        "mnist_losses, mnist_accuracies = mnistModel.fit(\n",
        "    mnist_train_images, mnist_train_vec_labels, epochs=10\n",
        ")\n",
        "end = time.time()\n",
        "print(\"Trainingsdauer: {:.1f}s\".format(end - start))\n",
        "\n",
        "mnist_val_loss, mnist_val_acc = mnistModel.evaluate(\n",
        "    validation_images=mnist_test_images, validation_labels=mnist_test_vec_labels\n",
        ")\n",
        "print(f\"Validation loss: {mnist_val_loss}\")\n",
        "print(f\"Validation accuracy: {mnist_val_acc}\")\n",
        "\n",
        "plot_loss_and_accuracy(mnist_losses, mnist_accuracies, xlabel=\"iterations\")\n",
        "\n",
        "helpers.plot_predictions(\n",
        "    mnistModel, mnist_test_images[:20], labels=mnist_test_vec_labels[:20])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}